{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Lab-01-MNIST-MLP-vs-CNN-Bakeoff.ipynb by pepper",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "gBh8COqpfeY_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PepperJao/Modeling/blob/master/CNN_Lab_01_MNIST_MLP_vs_CNN_Bakeoff_ipynb_by_pepper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcG4NkjfeXg",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HKNHm3PfeXh",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we're going to explore handwritten digit recognition task using MNIST database and CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKjOnZLgfeXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "227d8246-cd56-4b28-a92e-0776da6dd9da"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import backend as K\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "from IPython.display import SVG\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGGUFCwafeXm",
        "colab_type": "text"
      },
      "source": [
        "Set style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX-SV-cDfeXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
        "matplotlib.rcParams[\"figure.figsize\"] = (10, 8)\n",
        "matplotlib.rcParams[\"legend.framealpha\"] = 1\n",
        "matplotlib.rcParams[\"legend.frameon\"] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg4g8Ht1feXq",
        "colab_type": "text"
      },
      "source": [
        "Just for the sake of reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4i2YGNfeXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(41)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkhexmWCfeXu",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kxhyIDTfeXw",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial we're going to use MNIST dataset with handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgaPMeQHfeXx",
        "colab_type": "text"
      },
      "source": [
        "## MNIST overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH_qBWkifeX1",
        "colab_type": "text"
      },
      "source": [
        "Let's download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XueSv3kofeX2",
        "colab_type": "code",
        "outputId": "7b0d8220-37da-43d9-baaa-c578c5efe12d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# load MNIST data\n",
        "from sklearn.model_selection import train_test_split\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "img_rows, img_cols = 28, 28\n",
        "number_of_classes = 10\n",
        "print(f\"X before flatten train      shape: {X_train.shape}\")\n",
        "print(f\"X before flatten validation shape: {X_valid.shape}\")\n",
        "print(f\"X before flatten test       shape: {X_test.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "X before flatten train      shape: (48000, 28, 28)\n",
            "X before flatten validation shape: (12000, 28, 28)\n",
            "X before flatten test       shape: (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzjT7ThAfeX9",
        "colab_type": "code",
        "outputId": "07ce4eb7-d08f-4bb8-8e52-db8e2a630b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rn-IKG9feYD",
        "colab_type": "code",
        "outputId": "7dcaf9da-623e-41af-9de5-80f719a6fe79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "for num, i in enumerate(np.random.choice(len(X_train), 10)):\n",
        "    plt.subplot(2, 5, num + 1)\n",
        "    plt.imshow(X_train[i], cmap=\"Greys_r\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(str(y_train[i]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAE/CAYAAACq327HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxwAAAIDx/wDENgSkwY6BKgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hERBxfBxfeYI",
        "colab_type": "text"
      },
      "source": [
        "Let's see objects are distributed among classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Wqq3prfeYJ",
        "colab_type": "code",
        "outputId": "3b0ff69f-0929-44e6-f961-7fac45438674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "x_bars, y_bars = np.unique(y_train, return_counts=True)\n",
        "plt.bar(x_bars, y_bars)\n",
        "plt.xlim([-1, 10])\n",
        "plt.xticks(np.arange(0, 10))\n",
        "plt.xlabel(\"Digit\", fontsize=14)\n",
        "plt.ylabel(\"Number of pics\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAHwCAYAAAAy8g5CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weufLuhtZKyAgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SabILGnfeYP",
        "colab_type": "text"
      },
      "source": [
        "As one can see, the task is pretty balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIaTwOgUfeYS",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gOSVdUPfeYZ",
        "colab_type": "text"
      },
      "source": [
        "First of all, let's predefine image parameters:\n",
        "* **img_rows, img_cols** $-$ 2D dimension of a pictures; for MNIST it is $28 \\times 28$\n",
        "* **nb_classes** $-$ number of classes (digits in our case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzkDgMmMfeYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = 28, 28\n",
        "nb_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waU0FFqEfeYf",
        "colab_type": "text"
      },
      "source": [
        "Theano and Tensorflow both are tensor-based libraries. It means that all objects inside it, all inputs and outputs are **tensors**. One can treat tensor as a simple multidimensional array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNRY36NYfeYg",
        "colab_type": "text"
      },
      "source": [
        "The thing that is different in Theano and Tensorflow is order of these dimensions inside tensor.\n",
        "\n",
        "With Theano yo're going to have 4-dimensional tensor with the following dimensions: **(Objects, Channels, Image rows,Image columns)**. Assume that $\\text{X_train}$ is our tensor. Then $\\text{X_train}[0]$ gives you one trainig object - it is an image with few channels in general case. $\\text{X_train}[0][0]$ gives you the first channel of the first object. And so on. The logic of tensors should be clear now.\n",
        "\n",
        "In Tensorflow the order is the following: **(Objects, Image rows,Image columns, Channels)**\n",
        "\n",
        "Thus we need to check what dimension order do we have and reshape our tensor in accordance with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD-ysBCpfeYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if K.image_dim_ordering() == 'th':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-NL8VLnfeYj",
        "colab_type": "text"
      },
      "source": [
        "Here we have grayscale image and thus the number of the channels is $1$. Here I used Tensorflow library with the corresponding order of dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhTZsR0feYk",
        "colab_type": "code",
        "outputId": "d2295cd5-9c3f-4068-9082-952feb52a361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('X_train shape:', X_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (48000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF7iyaqtfeY2",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow prefers to work with $\\text{float32}$ data type. So the next step is to cast data. Also let's have our data in $[0; 1]$ interval $-$ it's common choice for grayscale images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyftj0MDfeY3",
        "colab_type": "code",
        "outputId": "a62fd57e-b2e8-4a3e-ffbd-754f63e2ea4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Here we're going to use dense baseline models so we need to represent our data as 1-dimensional vectors\n",
        "#So we flatten input images to 1D vectors \n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
        "X_test  = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Tensorflow/Keras prefers to work with float32 data type. \n",
        "# So the next step is to cast data. \n",
        "# Also let's have our data in [0; 1]interval; it's common choice for grayscale images.\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_valid = X_valid.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_valid /= 255\n",
        "X_test /= 255\n",
        "print(f\"X train      shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"X validation shape: {X_valid.shape},  {y_valid.shape}\")\n",
        "print(f\"X test       shape: {X_test.shape},  {y_test.shape}\")\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X train      shape: (48000, 784), (48000,)\n",
            "X validation shape: (12000, 784),  (12000,)\n",
            "X test       shape: (10000, 784),  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBh8COqpfeY_",
        "colab_type": "text"
      },
      "source": [
        "## Setup the MNIST data\n",
        "Setup the MNIST data. Here we use  **digits 0 to 9**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-c_06oLfeZD",
        "colab_type": "text"
      },
      "source": [
        "Convert labels into [One-Hot Encoding](https://en.wikipedia.org/wiki/One-hot) because we're going to learn them through the softmax layer of CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O19VXYhnfeZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(y_train, nb_classes)\n",
        "y_valid = to_categorical(y_valid, nb_classes)\n",
        "y_test  = to_categorical(y_test, nb_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfgJMXSXfeZI",
        "colab_type": "code",
        "outputId": "96317887-190d-4197-df46-1ea0ff8a650e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(f\"X train      shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"X validation shape: {X_valid.shape},  {y_valid.shape}\")\n",
        "print(f\"X test       shape: {X_test.shape},  {y_test.shape}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X train      shape: (48000, 784), (48000, 10)\n",
            "X validation shape: (12000, 784),  (12000, 10)\n",
            "X test       shape: (10000, 784),  (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfQ7a9ZyfeZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5e8KIVPfeZP",
        "colab_type": "text"
      },
      "source": [
        "# Dense baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMht4oEmfeZQ",
        "colab_type": "text"
      },
      "source": [
        "First of all, let's build MLP model and see how it performs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSE36rDVfeZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dense = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtvrTsMfeZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "68826d94-0fc6-4307-9dcc-8726971827d9"
      },
      "source": [
        "model_dense.add(Dense(128, input_shape=(img_rows * img_cols,), activation=\"relu\"))\n",
        "model_dense.add(Dropout(0.5))\n",
        "model_dense.add(Dense(128, activation=\"relu\"))\n",
        "model_dense.add(Dropout(0.5))\n",
        "model_dense.add(Dense(128, activation=\"relu\"))\n",
        "model_dense.add(Dropout(0.5))\n",
        "model_dense.add(Dense(nb_classes, activation=\"softmax\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3inIV0NLfeZX",
        "colab_type": "text"
      },
      "source": [
        "Our model the the following architercture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EtKOlTAfeZX",
        "colab_type": "code",
        "outputId": "6c39b013-154e-4ac5-ac0a-cdd024e9f3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "model_dense.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 134,794\n",
            "Trainable params: 134,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a36MB-XmfeZb",
        "colab_type": "code",
        "outputId": "a44dbb9c-8d44-4448-944d-da48b36b37a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "source": [
        "SVG(model_to_dot(model_dense, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"626pt\" viewBox=\"0.00 0.00 287.00 626.00\" width=\"287pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 622)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-622 283,-622 283,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139848773001624 -->\n<g class=\"node\" id=\"node1\">\n<title>139848773001624</title>\n<polygon fill=\"none\" points=\"13.5,-498.5 13.5,-544.5 265.5,-544.5 265.5,-498.5 13.5,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-517.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"120.5,-498.5 120.5,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"120.5,-521.5 178.5,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"178.5,-498.5 178.5,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-529.3\">(None, 784)</text>\n<polyline fill=\"none\" points=\"178.5,-521.5 265.5,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-506.3\">(None, 128)</text>\n</g>\n<!-- 139848772939112 -->\n<g class=\"node\" id=\"node2\">\n<title>139848772939112</title>\n<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 279,-461.5 279,-415.5 0,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-434.8\">dropout_1: Dropout</text>\n<polyline fill=\"none\" points=\"134,-415.5 134,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-438.5 192,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-415.5 192,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-446.3\">(None, 128)</text>\n<polyline fill=\"none\" points=\"192,-438.5 279,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-423.3\">(None, 128)</text>\n</g>\n<!-- 139848773001624&#45;&gt;139848772939112 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139848773001624-&gt;139848772939112</title>\n<path d=\"M139.5,-498.3799C139.5,-490.1745 139.5,-480.7679 139.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-471.784 139.5,-461.784 136.0001,-471.784 143.0001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139848772937208 -->\n<g class=\"node\" id=\"node3\">\n<title>139848772937208</title>\n<polygon fill=\"none\" points=\"13.5,-332.5 13.5,-378.5 265.5,-378.5 265.5,-332.5 13.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-351.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"120.5,-332.5 120.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"120.5,-355.5 178.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"178.5,-332.5 178.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-363.3\">(None, 128)</text>\n<polyline fill=\"none\" points=\"178.5,-355.5 265.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-340.3\">(None, 128)</text>\n</g>\n<!-- 139848772939112&#45;&gt;139848772937208 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139848772939112-&gt;139848772937208</title>\n<path d=\"M139.5,-415.3799C139.5,-407.1745 139.5,-397.7679 139.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-388.784 139.5,-378.784 136.0001,-388.784 143.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139848859524064 -->\n<g class=\"node\" id=\"node4\">\n<title>139848859524064</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 279,-295.5 279,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-268.8\">dropout_2: Dropout</text>\n<polyline fill=\"none\" points=\"134,-249.5 134,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-272.5 192,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-249.5 192,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-280.3\">(None, 128)</text>\n<polyline fill=\"none\" points=\"192,-272.5 279,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-257.3\">(None, 128)</text>\n</g>\n<!-- 139848772937208&#45;&gt;139848859524064 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139848772937208-&gt;139848859524064</title>\n<path d=\"M139.5,-332.3799C139.5,-324.1745 139.5,-314.7679 139.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-305.784 139.5,-295.784 136.0001,-305.784 143.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139848773005264 -->\n<g class=\"node\" id=\"node5\">\n<title>139848773005264</title>\n<polygon fill=\"none\" points=\"13.5,-166.5 13.5,-212.5 265.5,-212.5 265.5,-166.5 13.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-185.8\">dense_3: Dense</text>\n<polyline fill=\"none\" points=\"120.5,-166.5 120.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"120.5,-189.5 178.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"178.5,-166.5 178.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-197.3\">(None, 128)</text>\n<polyline fill=\"none\" points=\"178.5,-189.5 265.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-174.3\">(None, 128)</text>\n</g>\n<!-- 139848859524064&#45;&gt;139848773005264 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139848859524064-&gt;139848773005264</title>\n<path d=\"M139.5,-249.3799C139.5,-241.1745 139.5,-231.7679 139.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-222.784 139.5,-212.784 136.0001,-222.784 143.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139848772737848 -->\n<g class=\"node\" id=\"node6\">\n<title>139848772737848</title>\n<polygon fill=\"none\" points=\"0,-83.5 0,-129.5 279,-129.5 279,-83.5 0,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-102.8\">dropout_3: Dropout</text>\n<polyline fill=\"none\" points=\"134,-83.5 134,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-106.5 192,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-83.5 192,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-114.3\">(None, 128)</text>\n<polyline fill=\"none\" points=\"192,-106.5 279,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-91.3\">(None, 128)</text>\n</g>\n<!-- 139848773005264&#45;&gt;139848772737848 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139848773005264-&gt;139848772737848</title>\n<path d=\"M139.5,-166.3799C139.5,-158.1745 139.5,-148.7679 139.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-139.784 139.5,-129.784 136.0001,-139.784 143.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139848763663920 -->\n<g class=\"node\" id=\"node7\">\n<title>139848763663920</title>\n<polygon fill=\"none\" points=\"13.5,-.5 13.5,-46.5 265.5,-46.5 265.5,-.5 13.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-19.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"120.5,-.5 120.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"120.5,-23.5 178.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"178.5,-.5 178.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-31.3\">(None, 128)</text>\n<polyline fill=\"none\" points=\"178.5,-23.5 265.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-8.3\">(None, 10)</text>\n</g>\n<!-- 139848772737848&#45;&gt;139848763663920 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139848772737848-&gt;139848763663920</title>\n<path d=\"M139.5,-83.3799C139.5,-75.1745 139.5,-65.7679 139.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-56.784 139.5,-46.784 136.0001,-56.784 143.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139848773003920 -->\n<g class=\"node\" id=\"node8\">\n<title>139848773003920</title>\n<polygon fill=\"none\" points=\"75,-581.5 75,-617.5 204,-617.5 204,-581.5 75,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"139.5\" y=\"-595.8\">139848773003920</text>\n</g>\n<!-- 139848773003920&#45;&gt;139848773001624 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139848773003920-&gt;139848773001624</title>\n<path d=\"M139.5,-581.4092C139.5,-573.4308 139.5,-563.795 139.5,-554.606\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"143.0001,-554.5333 139.5,-544.5333 136.0001,-554.5334 143.0001,-554.5333\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUmNVe9efeZg",
        "colab_type": "text"
      },
      "source": [
        "Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sPy7u8HfeZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dense.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='adam',\n",
        "                    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zVWA8oDfeZk",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSi1irBfeZk",
        "colab_type": "code",
        "outputId": "794da553-8dac-42af-df25-55fd355faee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "epochs=20\n",
        "hist = model_dense.fit(X_train.reshape((len(X_train), img_cols * img_rows)), y_train, \n",
        "                       validation_data = (X_valid, y_valid), \n",
        "                       epochs=epochs, batch_size=128)\n",
        "end = time.time()\n",
        "seconds_per_epoch = f\"{(end - start)/epochs:.4}\"\n",
        "print(f\"seconds_per_epoch: {seconds_per_epoch}\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 4s 83us/step - loss: 1.5392 - acc: 0.4413 - val_loss: 4.2871 - val_acc: 0.7167\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 0.8231 - acc: 0.7227 - val_loss: 4.2131 - val_acc: 0.7295\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 0.6430 - acc: 0.8014 - val_loss: 3.9427 - val_acc: 0.7505\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 0.5500 - acc: 0.8359 - val_loss: 3.3898 - val_acc: 0.7848\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 0.4966 - acc: 0.8534 - val_loss: 2.8373 - val_acc: 0.8207\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 0.4551 - acc: 0.8676 - val_loss: 2.4442 - val_acc: 0.8458\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 0.4244 - acc: 0.8783 - val_loss: 2.2522 - val_acc: 0.8576\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 0.4016 - acc: 0.8849 - val_loss: 2.1146 - val_acc: 0.8663\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 0.3795 - acc: 0.8911 - val_loss: 2.0521 - val_acc: 0.8704\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 0.3602 - acc: 0.8980 - val_loss: 1.9754 - val_acc: 0.8754\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 0.3482 - acc: 0.9010 - val_loss: 1.7963 - val_acc: 0.8862\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 0.3418 - acc: 0.9031 - val_loss: 1.6858 - val_acc: 0.8937\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.3311 - acc: 0.9062 - val_loss: 1.6970 - val_acc: 0.8926\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 3s 64us/step - loss: 0.3184 - acc: 0.9097 - val_loss: 1.7348 - val_acc: 0.8906\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.3078 - acc: 0.9120 - val_loss: 1.6803 - val_acc: 0.8933\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 0.3107 - acc: 0.9129 - val_loss: 1.5212 - val_acc: 0.9037\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 0.2985 - acc: 0.9159 - val_loss: 1.4988 - val_acc: 0.9050\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 3s 63us/step - loss: 0.2964 - acc: 0.9169 - val_loss: 1.4658 - val_acc: 0.9069\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 3s 65us/step - loss: 0.2873 - acc: 0.9186 - val_loss: 1.5158 - val_acc: 0.9043\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 3s 66us/step - loss: 0.2823 - acc: 0.9211 - val_loss: 1.5438 - val_acc: 0.9024\n",
            "seconds_per_epoch: 3.268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G0kpoA5feZn",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRpkj3U0feZo",
        "colab_type": "code",
        "outputId": "4fd062c6-f0eb-4a6d-d4fb-e201de6a4c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Returns the loss value & metrics values for the model in test mode.\n",
        "# CXE and accuracy\n",
        "model_dense.evaluate(X_test,  y_test, verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 46us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1756887694209814, 0.9497]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK_FzPGDfeZq",
        "colab_type": "text"
      },
      "source": [
        "###  Visualize the learning, epoch by epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrUAXqtNfeZq",
        "colab_type": "code",
        "outputId": "0dd04125-d729-42b4-f48a-16cfbd64990b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.suptitle(\"Dense model training\", fontsize=18)\n",
        "plt.subplot(121)\n",
        "plt.plot(hist.history[\"loss\"], label=\"Train\")\n",
        "plt.plot(hist.history[\"val_loss\"], label=\"Validation\")\n",
        "plt.grid(\"on\")\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"Crossentropy\", fontsize=14)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title(\"Crossentropy CXE\")\n",
        "plt.subplot(122)\n",
        "plt.plot(hist.history[\"acc\"], label=\"Train\")\n",
        "plt.grid(\"on\")\n",
        "plt.plot(hist.history[\"val_acc\"], label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"Accuracy\")\n",
        "\n",
        "#plt.ylim([0.88, 1.0]);"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKIAAAIkCAYAAAAtXYN1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwUQUERERERERERHZxf8DMO2m60de0MwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMQntr-lfeZt",
        "colab_type": "text"
      },
      "source": [
        "Table to store the results of the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or29A_E2feZu",
        "colab_type": "code",
        "outputId": "8f3c5e4c-f28a-49df-f56d-fd6a572acb64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "\n",
        "\n",
        "expLog"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp_name</th>\n",
              "      <th>epoch (secs)</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Train CXE Loss</th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Validation CXE Loss</th>\n",
              "      <th>Validation  Acc</th>\n",
              "      <th>Test CXE Loss</th>\n",
              "      <th>Test  Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [exp_name, epoch (secs), Epochs, Train CXE Loss, Train Acc, Validation CXE Loss, Validation  Acc, Test CXE Loss, Test  Accuracy]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86vvD2CnfeZz",
        "colab_type": "text"
      },
      "source": [
        "### Debugging (please use)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D71xNisMfeZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = 3 \n",
        "y =4\n",
        "\n",
        "from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "\n",
        "print(f\"x: {x}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "PeYXdeXKfeZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the result of this experiment to the log book\n",
        "exp_name = \"MLP-784-128-128-128-10\" # experiment name\n",
        "#del expLog\n",
        "try:\n",
        "    expLog\n",
        "except NameError:\n",
        "    expLog = pd.DataFrame(columns=[\"exp_name\", \"epoch (secs)\", \"Epochs\", \"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                    \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "    \n",
        "# Add a experiment results to the experiment log\n",
        "model = model_dense\n",
        "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "expLog.loc[len(expLog)] = [f\"{exp_name}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train, y_train, verbose=0), \n",
        "                   model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                   model.evaluate(X_test,  y_test, verbose=1)], -1), 3))\n",
        "expLog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbXFWe2yvPos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUaDcrTtfeZ8",
        "colab_type": "text"
      },
      "source": [
        "## Task 3.1 : choose a better MLP architecture\n",
        "\n",
        "\n",
        "\n",
        "Using all the training data, search for a good CNN architecture to model the MNIST problem. Above we explored 784-128-128-128-10.\n",
        " \n",
        "\n",
        "It is up to you what the model will be. Here are some things you need to decide:\n",
        "1. how many DENSE layers?\n",
        "* how manu neurons per layer\n",
        "* report the accuracy on the test data set\n",
        "\n",
        "Please use use like this to explore different architectures:\n",
        "\n",
        "\n",
        "```python\n",
        "model_dense = Sequential()  #784-12-12-10\n",
        "model_dense.add(Dense(12, input_shape=(img_rows * img_cols,), activation=\"relu\"))\n",
        "model_dense.add(Dense(12, activation=\"relu\"))\n",
        "model_dense.add(Dense(nb_classes, activation=\"softmax\"))\n",
        "\n",
        "# E.g., explore 24 possible architectures \n",
        "number_of_hidden_layes = [1, 2, 3, 4]\n",
        "number_of_hidden_nodes = [12, 24, 48, 96, 128,256]\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Please paste your response here using the following format:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n",
        "\n",
        "\n",
        "## Task 3.2 : with 500 training examples\n",
        "\n",
        "Use only 500 examples per class (recall we  have only 10 classes) and find the best architecture. \n",
        "HINTs:\n",
        "* you can do  stratified sampling OR just random selection of 5,000 examples\n",
        "* for inspiration, see [here](https://mclguide.readthedocs.io/en/latest/sklearn/multiclass.html)\n",
        "* For more details see: [sklearn.model_selection.train_test_split manual page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "  * `stratify` : array-like or None (default=None)\n",
        "    If not None, data is split in a stratified fashion, using this as the class labels.\n",
        "\n",
        "```python\n",
        "# load MNIST data\n",
        "from sklearn.model_selection import train_test_split\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
        "                                                     test_size=55000./60000., random_state=42, \n",
        "                                                     stratify=y_train)\n",
        "img_rows, img_cols = 28, 28\n",
        "number_of_classes = 10\n",
        "print(f\"X before flatten train      shape: {X_train.shape}\")\n",
        "print(f\"X before flatten validation shape: {X_valid.shape}\")\n",
        "print(f\"X before flatten test       shape: {X_test.shape}\")\n",
        "print(f\"All data: {np.bincount(y_train) / float(len(y_train))}\")\n",
        "```\n",
        "\n",
        "**Please paste your response here:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n",
        "\n",
        "## Task 3.3 : Odd versus Even Classifier\n",
        "\n",
        "Build a binary classifier that classifiers odd numbers versus even numbers. Again, try to find the best architecture.\n",
        "\n",
        "\n",
        "**Please paste your response here:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n",
        "\n",
        "\n",
        "## 3.4 Homework Submission Instructions:\n",
        "\n",
        "\n",
        "\n",
        "1. Please fill in your best architecture for each of the three homework tasks 3.1, 3.2, 3.3: \n",
        "https://docs.google.com/spreadsheets/d/11Ot_v7EKmBoNgXBEjCsN_S7tYtfa5OXFLQoH62JCf1g/edit?usp=sharing\n",
        "  * NOTE: that each task has a SEPARATE table/tab in the above spreadsheet.\n",
        "  * The winner in each task gets a special prize!\n",
        "* Please upload your notebook to the following Google Drive folder (you save directly or just can drag and drop):\n",
        "  * https://drive.google.com/drive/folders/18Ng3PfnVBhrT82s_rQ_-WylE5rN7f9ej?usp=sharing\n",
        "* Your homework submission is now due tomorrow (Thursday) morning at 8AM (that will give you an extra 12 hours!!)\n",
        "* Please send the following by EMAIL to James.Shanahan@gmail.com with the subject \"Sichuan2019: MNIST Best MLP Architecture\"\n",
        "\n",
        "  * Link to your own Google Colab notebook where you did the experiments\n",
        "  * All the text + your response from this cell\n",
        "  * A screenshot of your expLog table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y-uv7zu85TrZ"
      },
      "source": [
        "##Homework by Pepper\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vereTAZGY8dN",
        "colab_type": "text"
      },
      "source": [
        "### Task 3.1 : choose a better MLP architecture\n",
        "\n",
        "\n",
        "\n",
        "Using all the training data, search for a good CNN architecture to model the MNIST problem. Above we explored 784-128-128-128-10.\n",
        " \n",
        "\n",
        "It is up to you what the model will be. Here are some things you need to decide:\n",
        "1. how many DENSE layers?\n",
        "```python\n",
        "'3 dense layers'\n",
        "'1 hidden layers'\n",
        "```\n",
        "* how manu neurons per layer\n",
        "```python\n",
        "'256 per layer'\n",
        "```\n",
        "* report the accuracy on the test data set\n",
        "```python\n",
        "'784-256-10\t\taccuracy_train:0.99995\taccuracy_test:0.982'\n",
        "```\n",
        "**Please paste your response here using the following format:** \n",
        "* Best archtecture is:  \n",
        "```python\n",
        "'784-256-10\t\t\n",
        "```\n",
        "* Test accuracy: \n",
        "```python\n",
        "'accuracy_train:0.99995\taccuracy_test:0.982'\n",
        "```\n",
        "* Training time per epoch in seconds is : \n",
        "```python\n",
        "'1secs'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULo_VfWUy-9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "'''\n",
        "#2nd version\n",
        "#def list_all_architecture():\n",
        "neurons=[256,128,64,12]\n",
        "for dense_layer in range(1,4):#list all achitecture using 2 for loops\n",
        "  for n in range(1,4):\n",
        "    architecture = '784-'+str(neurons[n]) #architecture name     \n",
        "    \n",
        "    model_dense = Sequential()  \n",
        "    model_dense.add(Dense(neurons[n], input_shape=(img_rows * img_cols,), activation=\"relu\"))\n",
        "    \n",
        "    \n",
        "    \n",
        "    while n < dense_layer:#pair other nodes using while loop\n",
        "      architecture+='-'+str(neurons[n+1])  #architecture name   \n",
        "      \n",
        "      model_dense.add(Dense(neurons[n+1], activation=\"relu\"))\n",
        "      time+=1\n",
        " ''' \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "outputId": "8419f7e6-1c3e-4995-b8c9-c571e6179e84",
        "id": "bdQqzhV35Tra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        }
      },
      "source": [
        "\n",
        "#written by Pepper before 5pm Tuesday\n",
        "\n",
        "results = pd.DataFrame(columns=[\"Architecture\", \"Epoch time\", \"Accuracy Train\", \"Accuracy Test\", \"Loss Train\", \"Loss Test\"])\n",
        "\n",
        "for dense_layer in [4]:#[4,3,2,1]:\n",
        "  for neuron in [256]:#[256,128,64,12]:\n",
        "    architecture = '784-'+str(neuron)  \n",
        "    \n",
        "    model_dense = Sequential()  \n",
        "    model_dense.add(Dense(neuron, input_shape=(img_rows * img_cols,), activation=\"relu\"))\n",
        "    \n",
        "    time = 1\n",
        "    while time < dense_layer:#pair other nodes using while loop\n",
        "      architecture+='-'+str(neuron)   \n",
        "      model_dense.add(Dense(neuron, activation=\"relu\"))\n",
        "      time+=1\n",
        "             \n",
        "    model_dense.add(Dense(nb_classes, activation=\"softmax\"))\n",
        "    architecture+=\"-10\"\n",
        "      \n",
        "    #compile before training & testing  \n",
        "    model_dense.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    #training & testing (thought this was plotting and deleted, and got 0.01% of accuracy)\n",
        "    import time\n",
        "    start = time.time()\n",
        "    epochs=20\n",
        "    hist = model_dense.fit(X_train.reshape((len(X_train), img_cols * img_rows)), y_train, \n",
        "                          validation_data = (X_valid, y_valid), \n",
        "                       epochs=epochs, batch_size=128)\n",
        "    end = time.time()\n",
        "    seconds_per_epoch = f\"{(end - start)/epochs:.4}\"\n",
        "    print(f\"seconds_per_epoch: {seconds_per_epoch}\")\n",
        "\n",
        "  #  hist = model_dense.fit(X_train.reshape((len(X_train), img_cols * img_rows)), y_train, \n",
        "   #       validation_data = (X_test.reshape((len(X_test), img_cols * img_rows)), y_test), \n",
        "    #      epochs=20, batch_size=128)\n",
        "\n",
        "    \n",
        "    \n",
        "    score_train = model_dense.evaluate(X_train.reshape((len(X_train), img_cols * img_rows)), y_train, verbose=0)\n",
        "    score_test = model_dense.evaluate(X_test.reshape((len(X_test), img_cols * img_rows)), y_test, verbose=0)\n",
        "    results.loc[len(results)] = np.array([architecture, \"1secs\", score_train[1], score_test[1], score_train[0], score_test[0]])\n",
        "    #results.index = [\"MLP\"]\n",
        "\n",
        "results\n",
        "      \n",
        "try:\n",
        "    expLog\n",
        "except NameError:\n",
        "    expLog = pd.DataFrame(columns=[\"exp_name\", \"epoch (secs)\", \"Epochs\", \"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                    \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "    \n",
        "# Add a experiment results to the experiment log\n",
        "model = model_dense\n",
        "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "expLog.loc[len(expLog)] = [f\"{architecture}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train, y_train, verbose=0), \n",
        "                   model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                   model.evaluate(X_test,  y_test, verbose=1)], -1), 3))\n",
        "expLog"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "48000/48000 [==============================] - 9s 193us/step - loss: 0.8720 - acc: 0.7083 - val_loss: 2.8366 - val_acc: 0.8170\n",
            "Epoch 2/20\n",
            "48000/48000 [==============================] - 6s 119us/step - loss: 0.3648 - acc: 0.8879 - val_loss: 2.4304 - val_acc: 0.8455\n",
            "Epoch 3/20\n",
            "48000/48000 [==============================] - 6s 119us/step - loss: 0.2677 - acc: 0.9185 - val_loss: 2.1169 - val_acc: 0.8651\n",
            "Epoch 4/20\n",
            "48000/48000 [==============================] - 6s 120us/step - loss: 0.2118 - acc: 0.9355 - val_loss: 1.4516 - val_acc: 0.9076\n",
            "Epoch 5/20\n",
            "48000/48000 [==============================] - 6s 122us/step - loss: 0.1708 - acc: 0.9484 - val_loss: 1.0734 - val_acc: 0.9313\n",
            "Epoch 6/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.1434 - acc: 0.9563 - val_loss: 1.1232 - val_acc: 0.9287\n",
            "Epoch 7/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.1233 - acc: 0.9628 - val_loss: 0.9806 - val_acc: 0.9373\n",
            "Epoch 8/20\n",
            "48000/48000 [==============================] - 6s 119us/step - loss: 0.1054 - acc: 0.9684 - val_loss: 0.9587 - val_acc: 0.9392\n",
            "Epoch 9/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.0940 - acc: 0.9719 - val_loss: 0.9555 - val_acc: 0.9390\n",
            "Epoch 10/20\n",
            "48000/48000 [==============================] - 6s 129us/step - loss: 0.0829 - acc: 0.9755 - val_loss: 0.9738 - val_acc: 0.9384\n",
            "Epoch 11/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.0774 - acc: 0.9763 - val_loss: 1.0556 - val_acc: 0.9334\n",
            "Epoch 12/20\n",
            "48000/48000 [==============================] - 6s 123us/step - loss: 0.0724 - acc: 0.9781 - val_loss: 1.0422 - val_acc: 0.9338\n",
            "Epoch 13/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.0629 - acc: 0.9807 - val_loss: 1.1092 - val_acc: 0.9300\n",
            "Epoch 14/20\n",
            "48000/48000 [==============================] - 6s 122us/step - loss: 0.0580 - acc: 0.9821 - val_loss: 1.3402 - val_acc: 0.9153\n",
            "Epoch 15/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.0528 - acc: 0.9836 - val_loss: 1.1762 - val_acc: 0.9257\n",
            "Epoch 16/20\n",
            "48000/48000 [==============================] - 6s 120us/step - loss: 0.0478 - acc: 0.9845 - val_loss: 1.1817 - val_acc: 0.9257\n",
            "Epoch 17/20\n",
            "48000/48000 [==============================] - 6s 119us/step - loss: 0.0431 - acc: 0.9863 - val_loss: 1.4726 - val_acc: 0.9074\n",
            "Epoch 18/20\n",
            "48000/48000 [==============================] - 6s 122us/step - loss: 0.0444 - acc: 0.9858 - val_loss: 1.1960 - val_acc: 0.9249\n",
            "Epoch 19/20\n",
            "48000/48000 [==============================] - 6s 120us/step - loss: 0.0391 - acc: 0.9873 - val_loss: 1.6321 - val_acc: 0.8976\n",
            "Epoch 20/20\n",
            "48000/48000 [==============================] - 6s 121us/step - loss: 0.0355 - acc: 0.9884 - val_loss: 1.1951 - val_acc: 0.9247\n",
            "seconds_per_epoch: 6.004\n",
            "10000/10000 [==============================] - 1s 86us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp_name</th>\n",
              "      <th>epoch (secs)</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Train CXE Loss</th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Validation CXE Loss</th>\n",
              "      <th>Validation  Acc</th>\n",
              "      <th>Test CXE Loss</th>\n",
              "      <th>Test  Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP-784-128-128-128-10</td>\n",
              "      <td>3.268</td>\n",
              "      <td>20</td>\n",
              "      <td>2.271</td>\n",
              "      <td>0.140</td>\n",
              "      <td>2.705</td>\n",
              "      <td>0.279</td>\n",
              "      <td>2.242</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>784-12-10</td>\n",
              "      <td>1.471</td>\n",
              "      <td>20</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.878</td>\n",
              "      <td>4.225</td>\n",
              "      <td>0.732</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>784-256-256-256-256-10</td>\n",
              "      <td>6.004</td>\n",
              "      <td>20</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.987</td>\n",
              "      <td>1.195</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.968</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 exp_name epoch (secs) Epochs  Train CXE Loss  Train Acc  \\\n",
              "0  MLP-784-128-128-128-10        3.268     20           2.271      0.140   \n",
              "1               784-12-10        1.471     20           0.438      0.878   \n",
              "2  784-256-256-256-256-10        6.004     20           0.039      0.987   \n",
              "\n",
              "   Validation CXE Loss  Validation  Acc  Test CXE Loss  Test  Accuracy  \n",
              "0                2.705            0.279          2.242           0.000  \n",
              "1                4.225            0.732          0.428           0.882  \n",
              "2                1.195            0.925          0.120           0.968  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctkLSemhyohz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    try:\n",
        "        expLog\n",
        "    except NameError:\n",
        "        expLog = pd.DataFrame(columns=[\"exp_name\", \"epoch (secs)\", \"Epochs\", \"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                        \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "\n",
        "    # Add a experiment results to the experiment log\n",
        "    model = model_dense\n",
        "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "    expLog.loc[len(expLog)] = [f\"{architecture}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train_selected, y_train_selected, verbose=0), \n",
        "                       model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                       model.evaluate(X_test_selected,  y_test_selected, verbose=1)], -1), 3))\n",
        "    expLog\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDvLetT_g6Bd",
        "colab_type": "text"
      },
      "source": [
        "###Task 3.2 : with 500 training examples\n",
        "\n",
        "Use only 500 examples per class (recall we  have only 10 classes) and find the best architecture. (HINT you can do  stratified sampling OR just random selxtion of 5,000 examples)\n",
        "\n",
        "**Please paste your response here:** \n",
        "* Best archtecture is:  \n",
        "784-256-256-256-256-10\t\n",
        "* Test accuracy: \n",
        "\n",
        " 0.8731\n",
        "* Training time per epoch in seconds is : \n",
        "\n",
        " 1secs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7tVKKJD7SdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "\n",
        "#X_train & y_train are matrices\n",
        "list_random=np.random.random(500)*59999\n",
        "#list_random=np.random.randint(2,len(X_train)-1)\n",
        "for matrix in list_random:\n",
        "  X_train_selected=X_train[0:1]\n",
        "  y_train_selected=y_train[0:1]\n",
        "  matrix= int(round(matrix))\n",
        "  #print(len(list_random))\n",
        "  #print(matrix)\n",
        "  #print(X_train[matrix:matrix+1])\n",
        "  X_train_selected=np.append(X_train_selected,X_train[matrix:matrix+1],axis=0)\n",
        "  y_train_selected=np.append(y_train_selected,y_train[matrix:matrix+1],axis=0)\n",
        "\n",
        "print(len(X_train_selected))\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9_OOZGyy_mD",
        "colab_type": "code",
        "outputId": "d8d8d9fa-6b9e-46fd-c01b-a32596402f07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5695
        }
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "#randomly select\n",
        "\n",
        "X_train_selected=X_train[0:1]\n",
        "y_train_selected=y_train[0:1]\n",
        "X_test_selected=X_test[0:1]\n",
        "y_test_selected=y_test[0:1]\n",
        "#list_random=np.random.random(5)*(len(X_train)-1)\n",
        "i=0\n",
        "while(i<500):\n",
        "\n",
        "    metrix=np.random.randint(2,59999)\n",
        "    X_train_selected=np.append(X_train_selected,X_train[metrix:metrix+1],axis=0)\n",
        "    y_train_selected=np.append(y_train_selected,y_train[metrix:metrix+1],axis=0)\n",
        "    i=i+1\n",
        "i=0\n",
        "while(i<100):\n",
        "    i=i+1\n",
        "    metrix2=np.random.randint(2,10000)\n",
        "    X_test_selected=np.append(X_test_selected,X_test[metrix:metrix+1],axis=0)\n",
        "    y_test_selected=np.append(y_test_selected,y_test[metrix:metrix+1],axis=0)\n",
        "    \n",
        "#print(len(X_train_selected))\n",
        "\n",
        "#train\n",
        "results = pd.DataFrame(columns=[\"Architecture\", \"Epoch time\", \"Accuracy Train\", \"Accuracy Test\", \"Loss Train\", \"Loss Test\"])\n",
        "\n",
        "\n",
        "for dense_layer in [4,3,2,1]:\n",
        "  for neuron in [256,128,64,12]:\n",
        "    architecture = '784-'+str(neuron)  \n",
        "    \n",
        "    model_dense = Sequential()  \n",
        "    model_dense.add(Dense(neuron, input_shape=(img_rows * img_cols,), activation=\"relu\"))\n",
        "    \n",
        "    time = 1\n",
        "    while time < dense_layer:#pair other nodes using while loop\n",
        "      architecture+='-'+str(neuron)   \n",
        "      model_dense.add(Dense(neuron, activation=\"relu\"))\n",
        "      time+=1\n",
        "             \n",
        "\n",
        "      \n",
        "      \n",
        "    model_dense.add(Dense(nb_classes, activation=\"softmax\"))\n",
        "    architecture+=\"-10\"\n",
        "      \n",
        "    #compile before training & testing  \n",
        "    model_dense.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    #training & testing (thought this was plotting and deleted, and got 0.01% of accuracy)\n",
        "  #  hist = model_dense.fit(X_train_selected.reshape((len(X_train_selected), img_cols * img_rows)), y_train_selected, \n",
        "   #       validation_data = (X_test_selected.reshape((len(X_test_selected), img_cols * img_rows)), y_test_selected), \n",
        "    #      epochs=20, batch_size=128)\n",
        "    import time\n",
        "    start = time.time()\n",
        "    epochs=20\n",
        "    hist = model_dense.fit(X_train_selected.reshape((len(X_train_selected), img_cols * img_rows)), y_train_selected, \n",
        "                          validation_data = (X_valid, y_valid), \n",
        "                       epochs=epochs, batch_size=128)\n",
        "    end = time.time()\n",
        "    seconds_per_epoch = f\"{(end - start)/epochs:.4}\"\n",
        "    print(f\"seconds_per_epoch: {seconds_per_epoch}\")\n",
        "\n",
        "    \n",
        "    \n",
        "    score_train = model_dense.evaluate(X_train_selected.reshape((len(X_train_selected), img_cols * img_rows)), y_train_selected, verbose=0)\n",
        "    score_test = model_dense.evaluate(X_test_selected.reshape((len(X_test_selected), img_cols * img_rows)), y_test_selected, verbose=0)\n",
        "\n",
        "    #results.loc[len(results)] = np.array([architecture, \"1secs\", score_train[1], score_test[1], score_train[0], score_test[0]])\n",
        "    #results.index = [\"MLP\"]\n",
        "\n",
        " #   results\n",
        "      \n",
        "try:\n",
        "    expLog\n",
        "except NameError:\n",
        "    expLog = pd.DataFrame(columns=[\"exp_name\", \"epoch (secs)\", \"Epochs\", \"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                    \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "    \n",
        "# Add a experiment results to the experiment log\n",
        "model = model_dense\n",
        "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "expLog.loc[len(expLog)] = [f\"{architecture}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train_selected, y_train_selected, verbose=0), \n",
        "                   model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                   model.evaluate(X_test_selected,  y_test_selected, verbose=1)], -1), 3))\n",
        "expLog      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 10ms/step - loss: 2.3023 - acc: 0.0720 - val_loss: 2.1821 - val_acc: 0.2162\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2989 - acc: 0.1183 - val_loss: 2.3016 - val_acc: 0.1784\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2947 - acc: 0.1183 - val_loss: 3.3952 - val_acc: 0.1568\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2887 - acc: 0.1183 - val_loss: 5.3537 - val_acc: 0.1268\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2831 - acc: 0.1183 - val_loss: 6.7310 - val_acc: 0.1719\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2770 - acc: 0.1183 - val_loss: 7.8529 - val_acc: 0.1663\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2650 - acc: 0.2262 - val_loss: 9.0632 - val_acc: 0.1331\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2514 - acc: 0.1362 - val_loss: 11.2023 - val_acc: 0.1114\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2337 - acc: 0.1183 - val_loss: 11.2525 - val_acc: 0.1301\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.1929 - acc: 0.2262 - val_loss: 10.8057 - val_acc: 0.1376\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.1507 - acc: 0.2159 - val_loss: 11.4452 - val_acc: 0.1229\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.1003 - acc: 0.2108 - val_loss: 11.9796 - val_acc: 0.1322\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.0342 - acc: 0.2108 - val_loss: 12.8790 - val_acc: 0.1354\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.9773 - acc: 0.2211 - val_loss: 11.9982 - val_acc: 0.1554\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.9305 - acc: 0.3342 - val_loss: 12.0765 - val_acc: 0.1614\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.8836 - acc: 0.2853 - val_loss: 12.5498 - val_acc: 0.1576\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.7975 - acc: 0.3188 - val_loss: 12.3495 - val_acc: 0.1737\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.7250 - acc: 0.3496 - val_loss: 11.6028 - val_acc: 0.2234\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.6665 - acc: 0.4602 - val_loss: 11.4964 - val_acc: 0.2487\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.6136 - acc: 0.4216 - val_loss: 11.3159 - val_acc: 0.2617\n",
            "seconds_per_epoch: 0.6285\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 10ms/step - loss: 2.3024 - acc: 0.1028 - val_loss: 2.2360 - val_acc: 0.1708\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 727us/step - loss: 2.3011 - acc: 0.1157 - val_loss: 2.1937 - val_acc: 0.1278\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 742us/step - loss: 2.2999 - acc: 0.0900 - val_loss: 2.1740 - val_acc: 0.1327\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 751us/step - loss: 2.2977 - acc: 0.0900 - val_loss: 2.2023 - val_acc: 0.1539\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 725us/step - loss: 2.2958 - acc: 0.0900 - val_loss: 2.3978 - val_acc: 0.1459\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 734us/step - loss: 2.2931 - acc: 0.0900 - val_loss: 2.9151 - val_acc: 0.1126\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 801us/step - loss: 2.2899 - acc: 0.0900 - val_loss: 3.1541 - val_acc: 0.1183\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 749us/step - loss: 2.2847 - acc: 0.0900 - val_loss: 3.1096 - val_acc: 0.2342\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 764us/step - loss: 2.2807 - acc: 0.2185 - val_loss: 3.8676 - val_acc: 0.2193\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 800us/step - loss: 2.2746 - acc: 0.2879 - val_loss: 5.3111 - val_acc: 0.1830\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 724us/step - loss: 2.2677 - acc: 0.2776 - val_loss: 6.7707 - val_acc: 0.1408\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 736us/step - loss: 2.2561 - acc: 0.3085 - val_loss: 7.6725 - val_acc: 0.1360\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 732us/step - loss: 2.2464 - acc: 0.2674 - val_loss: 9.2319 - val_acc: 0.1256\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 741us/step - loss: 2.2267 - acc: 0.2288 - val_loss: 11.3247 - val_acc: 0.1089\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 722us/step - loss: 2.1997 - acc: 0.2057 - val_loss: 12.1956 - val_acc: 0.1073\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 727us/step - loss: 2.1638 - acc: 0.2159 - val_loss: 12.3360 - val_acc: 0.1116\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 767us/step - loss: 2.1238 - acc: 0.2314 - val_loss: 12.3095 - val_acc: 0.1134\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 721us/step - loss: 2.0693 - acc: 0.2288 - val_loss: 12.3599 - val_acc: 0.1422\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 731us/step - loss: 2.0113 - acc: 0.2108 - val_loss: 11.8079 - val_acc: 0.2041\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 745us/step - loss: 1.9542 - acc: 0.2494 - val_loss: 11.3935 - val_acc: 0.2502\n",
            "seconds_per_epoch: 0.4868\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 10ms/step - loss: 2.3024 - acc: 0.0900 - val_loss: 2.2320 - val_acc: 0.2367\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 559us/step - loss: 2.3013 - acc: 0.1080 - val_loss: 2.1916 - val_acc: 0.2585\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 549us/step - loss: 2.3001 - acc: 0.1080 - val_loss: 2.1397 - val_acc: 0.2839\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 564us/step - loss: 2.2985 - acc: 0.1080 - val_loss: 2.0859 - val_acc: 0.2927\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 552us/step - loss: 2.2968 - acc: 0.1080 - val_loss: 2.0446 - val_acc: 0.2553\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 556us/step - loss: 2.2948 - acc: 0.1080 - val_loss: 2.0129 - val_acc: 0.2389\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 567us/step - loss: 2.2924 - acc: 0.1620 - val_loss: 2.0044 - val_acc: 0.2519\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 567us/step - loss: 2.2890 - acc: 0.2159 - val_loss: 2.0853 - val_acc: 0.2308\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 582us/step - loss: 2.2853 - acc: 0.2185 - val_loss: 2.2950 - val_acc: 0.2082\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 548us/step - loss: 2.2836 - acc: 0.1645 - val_loss: 2.4508 - val_acc: 0.2026\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 562us/step - loss: 2.2819 - acc: 0.1440 - val_loss: 2.4727 - val_acc: 0.2258\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 557us/step - loss: 2.2753 - acc: 0.2905 - val_loss: 2.4038 - val_acc: 0.2645\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 551us/step - loss: 2.2685 - acc: 0.3059 - val_loss: 2.7432 - val_acc: 0.2444\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 551us/step - loss: 2.2608 - acc: 0.2725 - val_loss: 3.0826 - val_acc: 0.2717\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 567us/step - loss: 2.2510 - acc: 0.2982 - val_loss: 3.5383 - val_acc: 0.2856\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 591us/step - loss: 2.2393 - acc: 0.3008 - val_loss: 4.4727 - val_acc: 0.2705\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 561us/step - loss: 2.2263 - acc: 0.3188 - val_loss: 5.9321 - val_acc: 0.2568\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 565us/step - loss: 2.2086 - acc: 0.2982 - val_loss: 7.0566 - val_acc: 0.2619\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 566us/step - loss: 2.1864 - acc: 0.2905 - val_loss: 8.1313 - val_acc: 0.2426\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 561us/step - loss: 2.1588 - acc: 0.3008 - val_loss: 9.3347 - val_acc: 0.2144\n",
            "seconds_per_epoch: 0.4206\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 10ms/step - loss: 2.3027 - acc: 0.0951 - val_loss: 2.2975 - val_acc: 0.1328\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 498us/step - loss: 2.3022 - acc: 0.1183 - val_loss: 2.2917 - val_acc: 0.1363\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 488us/step - loss: 2.3021 - acc: 0.1697 - val_loss: 2.2887 - val_acc: 0.1289\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 478us/step - loss: 2.3018 - acc: 0.1183 - val_loss: 2.2848 - val_acc: 0.1333\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 482us/step - loss: 2.3015 - acc: 0.2211 - val_loss: 2.2813 - val_acc: 0.1184\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 486us/step - loss: 2.3010 - acc: 0.1260 - val_loss: 2.2763 - val_acc: 0.1039\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 500us/step - loss: 2.3008 - acc: 0.1183 - val_loss: 2.2647 - val_acc: 0.1022\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 520us/step - loss: 2.3001 - acc: 0.1183 - val_loss: 2.2514 - val_acc: 0.1012\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 520us/step - loss: 2.2995 - acc: 0.1183 - val_loss: 2.2404 - val_acc: 0.1006\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 520us/step - loss: 2.2988 - acc: 0.1183 - val_loss: 2.2292 - val_acc: 0.1005\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 492us/step - loss: 2.2982 - acc: 0.1183 - val_loss: 2.2179 - val_acc: 0.1006\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 493us/step - loss: 2.2975 - acc: 0.1183 - val_loss: 2.2078 - val_acc: 0.1003\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 488us/step - loss: 2.2967 - acc: 0.1183 - val_loss: 2.1980 - val_acc: 0.1004\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 492us/step - loss: 2.2959 - acc: 0.1183 - val_loss: 2.1965 - val_acc: 0.1018\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 492us/step - loss: 2.2949 - acc: 0.1183 - val_loss: 2.2053 - val_acc: 0.1030\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 492us/step - loss: 2.2941 - acc: 0.1183 - val_loss: 2.2241 - val_acc: 0.1029\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 479us/step - loss: 2.2928 - acc: 0.1183 - val_loss: 2.2540 - val_acc: 0.1021\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 500us/step - loss: 2.2919 - acc: 0.1183 - val_loss: 2.3081 - val_acc: 0.1033\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 488us/step - loss: 2.2906 - acc: 0.1183 - val_loss: 2.3756 - val_acc: 0.1061\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 491us/step - loss: 2.2889 - acc: 0.1183 - val_loss: 2.4661 - val_acc: 0.1095\n",
            "seconds_per_epoch: 0.3987\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 11ms/step - loss: 2.3018 - acc: 0.1722 - val_loss: 2.1466 - val_acc: 0.1107\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2978 - acc: 0.1183 - val_loss: 2.5226 - val_acc: 0.1013\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2932 - acc: 0.1183 - val_loss: 3.9321 - val_acc: 0.0982\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2880 - acc: 0.1183 - val_loss: 5.9484 - val_acc: 0.0983\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2799 - acc: 0.1183 - val_loss: 8.7033 - val_acc: 0.0985\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2738 - acc: 0.1183 - val_loss: 11.1258 - val_acc: 0.0984\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2675 - acc: 0.1183 - val_loss: 11.2176 - val_acc: 0.1009\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2551 - acc: 0.1183 - val_loss: 10.0653 - val_acc: 0.1102\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2411 - acc: 0.1208 - val_loss: 9.7851 - val_acc: 0.1193\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2261 - acc: 0.1774 - val_loss: 10.0428 - val_acc: 0.1303\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.2022 - acc: 0.2031 - val_loss: 10.8981 - val_acc: 0.1281\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.1684 - acc: 0.2031 - val_loss: 10.9303 - val_acc: 0.1261\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.1288 - acc: 0.1902 - val_loss: 10.8403 - val_acc: 0.1256\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.0816 - acc: 0.2057 - val_loss: 10.9503 - val_acc: 0.1458\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.0451 - acc: 0.2416 - val_loss: 11.0667 - val_acc: 0.1538\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 2.0000 - acc: 0.3008 - val_loss: 11.0709 - val_acc: 0.1686\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 998us/step - loss: 1.9669 - acc: 0.2879 - val_loss: 11.7895 - val_acc: 0.1609\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.9602 - acc: 0.2725 - val_loss: 11.6356 - val_acc: 0.1777\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.9117 - acc: 0.2879 - val_loss: 11.6233 - val_acc: 0.1902\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 1ms/step - loss: 1.9063 - acc: 0.3342 - val_loss: 12.1671 - val_acc: 0.1729\n",
            "seconds_per_epoch: 0.6066\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 10ms/step - loss: 2.3022 - acc: 0.1234 - val_loss: 2.1891 - val_acc: 0.2160\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 726us/step - loss: 2.3003 - acc: 0.1774 - val_loss: 2.0971 - val_acc: 0.2636\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 733us/step - loss: 2.2984 - acc: 0.1491 - val_loss: 2.0555 - val_acc: 0.2288\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 738us/step - loss: 2.2959 - acc: 0.2057 - val_loss: 2.1868 - val_acc: 0.1938\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 730us/step - loss: 2.2921 - acc: 0.2339 - val_loss: 2.5878 - val_acc: 0.1657\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 727us/step - loss: 2.2880 - acc: 0.2314 - val_loss: 3.2299 - val_acc: 0.1621\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 730us/step - loss: 2.2835 - acc: 0.2262 - val_loss: 4.6957 - val_acc: 0.1306\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 738us/step - loss: 2.2778 - acc: 0.1362 - val_loss: 6.2311 - val_acc: 0.1217\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 722us/step - loss: 2.2703 - acc: 0.1260 - val_loss: 6.9202 - val_acc: 0.1310\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 738us/step - loss: 2.2619 - acc: 0.1414 - val_loss: 7.5062 - val_acc: 0.1333\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 740us/step - loss: 2.2517 - acc: 0.1697 - val_loss: 7.3650 - val_acc: 0.1534\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 726us/step - loss: 2.2405 - acc: 0.2185 - val_loss: 7.3194 - val_acc: 0.1848\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 729us/step - loss: 2.2260 - acc: 0.2005 - val_loss: 8.0604 - val_acc: 0.2060\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 723us/step - loss: 2.2069 - acc: 0.1620 - val_loss: 8.3684 - val_acc: 0.2197\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 725us/step - loss: 2.1840 - acc: 0.2596 - val_loss: 8.7932 - val_acc: 0.2178\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 713us/step - loss: 2.1572 - acc: 0.2005 - val_loss: 9.2311 - val_acc: 0.2037\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 736us/step - loss: 2.1297 - acc: 0.2108 - val_loss: 8.9858 - val_acc: 0.2101\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 720us/step - loss: 2.0969 - acc: 0.2442 - val_loss: 9.4083 - val_acc: 0.2137\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 714us/step - loss: 2.0670 - acc: 0.2879 - val_loss: 9.7766 - val_acc: 0.2263\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 728us/step - loss: 2.0403 - acc: 0.3625 - val_loss: 9.9912 - val_acc: 0.2361\n",
            "seconds_per_epoch: 0.4936\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 11ms/step - loss: 2.3024 - acc: 0.1080 - val_loss: 2.2583 - val_acc: 0.2538\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 568us/step - loss: 2.3014 - acc: 0.1851 - val_loss: 2.2184 - val_acc: 0.2671\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 572us/step - loss: 2.3002 - acc: 0.1362 - val_loss: 2.1773 - val_acc: 0.2738\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 578us/step - loss: 2.2993 - acc: 0.1183 - val_loss: 2.1334 - val_acc: 0.2876\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 567us/step - loss: 2.2984 - acc: 0.1954 - val_loss: 2.0892 - val_acc: 0.2966\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 571us/step - loss: 2.2971 - acc: 0.1414 - val_loss: 2.0579 - val_acc: 0.2728\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 558us/step - loss: 2.2957 - acc: 0.1183 - val_loss: 2.0359 - val_acc: 0.2755\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 571us/step - loss: 2.2942 - acc: 0.1465 - val_loss: 2.0342 - val_acc: 0.2838\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 563us/step - loss: 2.2924 - acc: 0.2108 - val_loss: 2.0587 - val_acc: 0.2763\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 571us/step - loss: 2.2905 - acc: 0.2211 - val_loss: 2.0806 - val_acc: 0.2820\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 564us/step - loss: 2.2889 - acc: 0.2339 - val_loss: 2.1895 - val_acc: 0.2657\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 572us/step - loss: 2.2865 - acc: 0.2108 - val_loss: 2.5113 - val_acc: 0.2196\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 562us/step - loss: 2.2838 - acc: 0.1568 - val_loss: 2.8831 - val_acc: 0.1970\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 562us/step - loss: 2.2809 - acc: 0.1208 - val_loss: 3.2790 - val_acc: 0.1925\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 570us/step - loss: 2.2773 - acc: 0.1234 - val_loss: 3.4727 - val_acc: 0.2145\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 561us/step - loss: 2.2721 - acc: 0.1440 - val_loss: 3.9575 - val_acc: 0.2117\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 562us/step - loss: 2.2672 - acc: 0.1697 - val_loss: 4.7255 - val_acc: 0.1884\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 565us/step - loss: 2.2608 - acc: 0.1491 - val_loss: 5.4585 - val_acc: 0.1786\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 568us/step - loss: 2.2539 - acc: 0.1902 - val_loss: 6.0258 - val_acc: 0.1847\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 570us/step - loss: 2.2462 - acc: 0.2237 - val_loss: 6.9113 - val_acc: 0.1847\n",
            "seconds_per_epoch: 0.4355\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n",
            "389/389 [==============================] - 4s 11ms/step - loss: 2.3026 - acc: 0.0900 - val_loss: 2.2919 - val_acc: 0.0853\n",
            "Epoch 2/20\n",
            "389/389 [==============================] - 0s 514us/step - loss: 2.3023 - acc: 0.0771 - val_loss: 2.2859 - val_acc: 0.0970\n",
            "Epoch 3/20\n",
            "389/389 [==============================] - 0s 513us/step - loss: 2.3023 - acc: 0.1131 - val_loss: 2.2818 - val_acc: 0.1003\n",
            "Epoch 4/20\n",
            "389/389 [==============================] - 0s 520us/step - loss: 2.3022 - acc: 0.1440 - val_loss: 2.2781 - val_acc: 0.1043\n",
            "Epoch 5/20\n",
            "389/389 [==============================] - 0s 515us/step - loss: 2.3020 - acc: 0.1183 - val_loss: 2.2748 - val_acc: 0.1090\n",
            "Epoch 6/20\n",
            "389/389 [==============================] - 0s 529us/step - loss: 2.3018 - acc: 0.1183 - val_loss: 2.2726 - val_acc: 0.1113\n",
            "Epoch 7/20\n",
            "389/389 [==============================] - 0s 511us/step - loss: 2.3015 - acc: 0.1183 - val_loss: 2.2712 - val_acc: 0.1145\n",
            "Epoch 8/20\n",
            "389/389 [==============================] - 0s 535us/step - loss: 2.3012 - acc: 0.1183 - val_loss: 2.2708 - val_acc: 0.1177\n",
            "Epoch 9/20\n",
            "389/389 [==============================] - 0s 523us/step - loss: 2.3010 - acc: 0.1183 - val_loss: 2.2714 - val_acc: 0.1211\n",
            "Epoch 10/20\n",
            "389/389 [==============================] - 0s 517us/step - loss: 2.3008 - acc: 0.1183 - val_loss: 2.2750 - val_acc: 0.1245\n",
            "Epoch 11/20\n",
            "389/389 [==============================] - 0s 520us/step - loss: 2.3005 - acc: 0.1183 - val_loss: 2.2818 - val_acc: 0.1312\n",
            "Epoch 12/20\n",
            "389/389 [==============================] - 0s 526us/step - loss: 2.3001 - acc: 0.1183 - val_loss: 2.2936 - val_acc: 0.1365\n",
            "Epoch 13/20\n",
            "389/389 [==============================] - 0s 540us/step - loss: 2.2998 - acc: 0.1183 - val_loss: 2.3124 - val_acc: 0.1337\n",
            "Epoch 14/20\n",
            "389/389 [==============================] - 0s 511us/step - loss: 2.2994 - acc: 0.1260 - val_loss: 2.3324 - val_acc: 0.1232\n",
            "Epoch 15/20\n",
            "389/389 [==============================] - 0s 511us/step - loss: 2.2990 - acc: 0.1311 - val_loss: 2.3505 - val_acc: 0.1159\n",
            "Epoch 16/20\n",
            "389/389 [==============================] - 0s 525us/step - loss: 2.2985 - acc: 0.1311 - val_loss: 2.3905 - val_acc: 0.1072\n",
            "Epoch 17/20\n",
            "389/389 [==============================] - 0s 527us/step - loss: 2.2981 - acc: 0.1722 - val_loss: 2.4574 - val_acc: 0.1009\n",
            "Epoch 18/20\n",
            "389/389 [==============================] - 0s 524us/step - loss: 2.2975 - acc: 0.2005 - val_loss: 2.5185 - val_acc: 0.0998\n",
            "Epoch 19/20\n",
            "389/389 [==============================] - 0s 520us/step - loss: 2.2968 - acc: 0.2108 - val_loss: 2.6068 - val_acc: 0.0998\n",
            "Epoch 20/20\n",
            "389/389 [==============================] - 0s 514us/step - loss: 2.2961 - acc: 0.2031 - val_loss: 2.7012 - val_acc: 0.1000\n",
            "seconds_per_epoch: 0.4431\n",
            "Train on 389 samples, validate on 12000 samples\n",
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItcQkVM2sddx",
        "colab_type": "code",
        "outputId": "17de322e-a381-4c4a-be54-e0aff5f576df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "expLog\n",
        "    try:\n",
        "        expLog\n",
        "    except NameError:\n",
        "        expLog = pd.DataFrame(columns=[\"exp_name\", \"epoch (secs)\", \"Epochs\", \"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                    \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "    \n",
        "\n",
        "# Add a experiment results to the experiment log\n",
        "    model = model_dense\n",
        "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "    expLog.loc[len(expLog)] = [f\"{architecture}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train_selected, y_train_selected, verbose=0), \n",
        "                   model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                   model.evaluate(X_test_selected,  y_test_selected, verbose=1)], -1), 3))\n",
        "    expLog  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp_name</th>\n",
              "      <th>epoch (secs)</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Train CXE Loss</th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Validation CXE Loss</th>\n",
              "      <th>Validation  Acc</th>\n",
              "      <th>Test CXE Loss</th>\n",
              "      <th>Test  Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [exp_name, epoch (secs), Epochs, Train CXE Loss, Train Acc, Validation CXE Loss, Validation  Acc, Test CXE Loss, Test  Accuracy]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "KOUZ9Caduk5N",
        "colab_type": "code",
        "outputId": "e650bd66-bf90-4b1d-8e41-db4586932501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "# add the result of this experiment to the log book\n",
        "exp_name = \"MLP-784-128-128-128-10\" # experiment name\n",
        "#del expLog\n",
        "try:\n",
        "    expLog\n",
        "except NameError:\n",
        "    expLog = pd.DataFrame(columns=[\"exp_name\", \"epoch (secs)\", \"Epochs\", \"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                    \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "    \n",
        "# Add a experiment results to the experiment log\n",
        "model = model_dense\n",
        "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "expLog.loc[len(expLog)] = [f\"{exp_name}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train_selected, y_train_selected, verbose=0), \n",
        "                   model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                   model.evaluate(X_test_selected,  y_test_selected, verbose=1)], -1), 3))\n",
        "expLog"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 0s 722us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp_name</th>\n",
              "      <th>epoch (secs)</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Train CXE Loss</th>\n",
              "      <th>Train Acc</th>\n",
              "      <th>Validation CXE Loss</th>\n",
              "      <th>Validation  Acc</th>\n",
              "      <th>Test CXE Loss</th>\n",
              "      <th>Test  Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP-784-128-128-128-10</td>\n",
              "      <td>3.268</td>\n",
              "      <td>20</td>\n",
              "      <td>2.271</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.705</td>\n",
              "      <td>0.279</td>\n",
              "      <td>2.242</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 exp_name epoch (secs) Epochs  Train CXE Loss  Train Acc  \\\n",
              "0  MLP-784-128-128-128-10        3.268     20           2.271       0.14   \n",
              "\n",
              "   Validation CXE Loss  Validation  Acc  Test CXE Loss  Test  Accuracy  \n",
              "0                2.705            0.279          2.242             0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXb9yT8tZLRj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Task 3.3 : Odd versus Even Classifier\n",
        "\n",
        "Build a binary classifier that classifiers odd numbers versus even numbers. Again, try to find the best architecture.\n",
        "\n",
        "\n",
        "**Please paste your response here:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n",
        "\n",
        "\n",
        "Please send the following by EMAIL to James.Shanahan@gmail.com with the subject \"Sichuan2019: MNIST Best MLP Architecture\"\n",
        "\n",
        "* Link to Coogle Colab notebook\n",
        "* all the text + your response from this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F61Twpw9h7B",
        "colab_type": "code",
        "outputId": "ff98ad46-95a4-4d0b-8dc1-7e38f365590e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11974
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "results = pd.DataFrame(columns=[\"Architecture\", \"Epoch time\", \"Accuracy Train\", \"Accuracy Test\", \"Loss Train\", \"Loss Test\"])\n",
        "\n",
        "\n",
        "for dense_layer in [4,3,2,1]:\n",
        "  for neuron in [256,128,64,12]:\n",
        "    architecture = '784-'+str(neuron)  \n",
        "    \n",
        "    model_dense = Sequential()  \n",
        "    model_dense.add(Dense(neuron, input_shape=(img_rows * img_cols,), activation=\"relu\"))\n",
        "    \n",
        "    time = 1\n",
        "    while time < dense_layer:#pair other nodes using while loop\n",
        "      architecture+='-'+str(neuron)   \n",
        "      model_dense.add(Dense(neuron, activation=\"relu\"))\n",
        "      time+=1\n",
        "    \n",
        "    \n",
        "    #exchange results in y_train from 1-10 to 1/0\n",
        "    #y_train manifying\n",
        "    even=y_train[1:2] #creat even & odd training data\n",
        "    odd=y_train[3:4]\n",
        "    for i in [ 5,2,13,17]:\n",
        "      even=np.append(even,y_train[i:i+1],axis=0)\n",
        "    for i in [ 7,0,15,4]:\n",
        "      odd=np.append(odd,y_train[i:i+1],axis=0)\n",
        "\n",
        "    y_train_classify=y_train[1:1]\n",
        "    i=0\n",
        "    while i < len(y_train):\n",
        "      if y_train[i:i+1] in even:\n",
        "        y_train_classify=np.append(y_train_classify,y_train[1:2],axis=0)\n",
        "      else:\n",
        "        y_train_classify=np.append(y_train_classify,y_train[3:4],axis=0)\n",
        "      i+=1\n",
        "      \n",
        "      \n",
        "    #y_test manifying   \n",
        "    i=0\n",
        "    y_test_classify=y_test[1:1]\n",
        "    while i < len(y_test):\n",
        "    \n",
        "      if y_test[i:i+1] in even:\n",
        "        y_test_classify=np.append(y_test_classify,y_train[1:2],axis=0)\n",
        "      else:\n",
        "        y_test_classify=np.append(y_test_classify,y_train[3:4],axis=0)\n",
        "      i+=1\n",
        "      \n",
        "      \n",
        "    model_dense.add(Dense(nb_classes, activation=\"softmax\"))\n",
        "    architecture+=\"-2\"\n",
        "      \n",
        "    #compile before training & testing  \n",
        "    model_dense.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    #training & testing (thought this was plotting and deleted, and got 0.01% of accuracy)\n",
        "    hist = model_dense.fit(X_train.reshape((len(X_train), img_cols * img_rows)), y_train_classify, \n",
        "          validation_data = (X_test.reshape((len(X_test), img_cols * img_rows)), y_test_classify), \n",
        "          epochs=20, batch_size=128)\n",
        "\n",
        "    \n",
        "    \n",
        "    score_train = model_dense.evaluate(X_train.reshape((len(X_train), img_cols * img_rows)), y_train_classify, verbose=0)\n",
        "    score_test = model_dense.evaluate(X_test.reshape((len(X_test), img_cols * img_rows)), y_test_classify, verbose=0)\n",
        "\n",
        "\n",
        "      \n",
        "    results.loc[len(results)] = np.array([architecture, \"1secs\", score_train[1], score_test[1], score_train[0], score_test[0]])\n",
        "    #results.index = [\"MLP\"]\n",
        "\n",
        "results\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0126 - acc: 0.9984 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0211 - acc: 0.9968 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2014e-07 - acc: 1.0000 - val_loss: 1.1947e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2014e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2014e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2013e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2012e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 1.2012e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2010e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 1.2009e-07 - acc: 1.0000 - val_loss: 1.1946e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2007e-07 - acc: 1.0000 - val_loss: 1.1945e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2004e-07 - acc: 1.0000 - val_loss: 1.1945e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 1.2002e-07 - acc: 1.0000 - val_loss: 1.1944e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 1.1999e-07 - acc: 1.0000 - val_loss: 1.1942e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0413 - acc: 0.9962 - val_loss: 3.7871e-06 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 1.7620e-06 - acc: 1.0000 - val_loss: 8.4366e-07 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 5.8515e-07 - acc: 1.0000 - val_loss: 4.1112e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.3480e-07 - acc: 1.0000 - val_loss: 2.7452e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 2.3993e-07 - acc: 1.0000 - val_loss: 2.1254e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.9379e-07 - acc: 1.0000 - val_loss: 1.7974e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.6811e-07 - acc: 1.0000 - val_loss: 1.6081e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.5258e-07 - acc: 1.0000 - val_loss: 1.4879e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.4270e-07 - acc: 1.0000 - val_loss: 1.4061e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.3613e-07 - acc: 1.0000 - val_loss: 1.3513e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 1.3156e-07 - acc: 1.0000 - val_loss: 1.3126e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.2834e-07 - acc: 1.0000 - val_loss: 1.2837e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 1.2608e-07 - acc: 1.0000 - val_loss: 1.2640e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 1.2441e-07 - acc: 1.0000 - val_loss: 1.2486e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 1.2321e-07 - acc: 1.0000 - val_loss: 1.2367e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.2228e-07 - acc: 1.0000 - val_loss: 1.2277e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.2159e-07 - acc: 1.0000 - val_loss: 1.2206e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.2106e-07 - acc: 1.0000 - val_loss: 1.2159e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 1.2068e-07 - acc: 1.0000 - val_loss: 1.2118e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 1.2039e-07 - acc: 1.0000 - val_loss: 1.2082e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.1942 - acc: 0.9189 - val_loss: 6.2871e-04 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 3.3612e-04 - acc: 1.0000 - val_loss: 1.7254e-04 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.1680e-04 - acc: 1.0000 - val_loss: 7.6804e-05 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 5.6568e-05 - acc: 1.0000 - val_loss: 4.0347e-05 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 2.8886e-05 - acc: 1.0000 - val_loss: 1.9483e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.3004e-05 - acc: 1.0000 - val_loss: 8.4843e-06 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 5.7503e-06 - acc: 1.0000 - val_loss: 4.0519e-06 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 2.8880e-06 - acc: 1.0000 - val_loss: 2.2325e-06 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.6450e-06 - acc: 1.0000 - val_loss: 1.3619e-06 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 1.0340e-06 - acc: 1.0000 - val_loss: 9.0223e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 7.0366e-07 - acc: 1.0000 - val_loss: 6.4172e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 5.1024e-07 - acc: 1.0000 - val_loss: 4.7962e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 3.8818e-07 - acc: 1.0000 - val_loss: 3.7336e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 3.0826e-07 - acc: 1.0000 - val_loss: 3.0231e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 2.5453e-07 - acc: 1.0000 - val_loss: 2.5327e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 2.1745e-07 - acc: 1.0000 - val_loss: 2.1820e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.9123e-07 - acc: 1.0000 - val_loss: 1.9303e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.7243e-07 - acc: 1.0000 - val_loss: 1.7480e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.5891e-07 - acc: 1.0000 - val_loss: 1.6165e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 1.4915e-07 - acc: 1.0000 - val_loss: 1.5178e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.0111 - acc: 0.9983 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 1.1929e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 1.1928e-07 - acc: 1.0000 - val_loss: 1.1928e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.0193 - acc: 0.9966 - val_loss: 9.0220e-07 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 8.6897e-07 - acc: 1.0000 - val_loss: 7.6842e-07 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 7.1851e-07 - acc: 1.0000 - val_loss: 6.1491e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 5.6699e-07 - acc: 1.0000 - val_loss: 4.7570e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 4.3564e-07 - acc: 1.0000 - val_loss: 3.6141e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 3.3148e-07 - acc: 1.0000 - val_loss: 2.7634e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 2.5773e-07 - acc: 1.0000 - val_loss: 2.1972e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 2.1013e-07 - acc: 1.0000 - val_loss: 1.8500e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 1.7970e-07 - acc: 1.0000 - val_loss: 1.6272e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 1.6037e-07 - acc: 1.0000 - val_loss: 1.4871e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 1.4774e-07 - acc: 1.0000 - val_loss: 1.3960e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 1.3931e-07 - acc: 1.0000 - val_loss: 1.3369e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 1.3366e-07 - acc: 1.0000 - val_loss: 1.2964e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 1.2972e-07 - acc: 1.0000 - val_loss: 1.2676e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 1.2696e-07 - acc: 1.0000 - val_loss: 1.2478e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 1.2502e-07 - acc: 1.0000 - val_loss: 1.2341e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 1.2359e-07 - acc: 1.0000 - val_loss: 1.2231e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 1.2256e-07 - acc: 1.0000 - val_loss: 1.2162e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 1.2177e-07 - acc: 1.0000 - val_loss: 1.2106e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 1.2121e-07 - acc: 1.0000 - val_loss: 1.2062e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.0204 - acc: 0.9991 - val_loss: 7.1594e-06 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 2.4295e-06 - acc: 1.0000 - val_loss: 1.4548e-06 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 6.8463e-07 - acc: 1.0000 - val_loss: 6.7831e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 3.6318e-07 - acc: 1.0000 - val_loss: 4.2689e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 2.4953e-07 - acc: 1.0000 - val_loss: 3.0874e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 1.9625e-07 - acc: 1.0000 - val_loss: 2.4332e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 1.6788e-07 - acc: 1.0000 - val_loss: 2.0471e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 1.5120e-07 - acc: 1.0000 - val_loss: 1.8012e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 1.4096e-07 - acc: 1.0000 - val_loss: 1.6390e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.3457e-07 - acc: 1.0000 - val_loss: 1.5299e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 3s 46us/step - loss: 1.3017e-07 - acc: 1.0000 - val_loss: 1.4500e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 1.2720e-07 - acc: 1.0000 - val_loss: 1.3931e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 1.2511e-07 - acc: 1.0000 - val_loss: 1.3511e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 1.2366e-07 - acc: 1.0000 - val_loss: 1.3194e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 1.2258e-07 - acc: 1.0000 - val_loss: 1.2954e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 1.2177e-07 - acc: 1.0000 - val_loss: 1.2760e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.2119e-07 - acc: 1.0000 - val_loss: 1.2620e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.2075e-07 - acc: 1.0000 - val_loss: 1.2500e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 1.2044e-07 - acc: 1.0000 - val_loss: 1.2411e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.2017e-07 - acc: 1.0000 - val_loss: 1.2334e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 0.2204 - acc: 0.9274 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 6.9238e-04 - acc: 1.0000 - val_loss: 3.4714e-04 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 2.3066e-04 - acc: 1.0000 - val_loss: 1.5406e-04 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.1204e-04 - acc: 1.0000 - val_loss: 8.4533e-05 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 6.4062e-05 - acc: 1.0000 - val_loss: 5.1726e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 4.0145e-05 - acc: 1.0000 - val_loss: 3.3880e-05 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 2.6619e-05 - acc: 1.0000 - val_loss: 2.3256e-05 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.8336e-05 - acc: 1.0000 - val_loss: 1.6423e-05 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 1.2969e-05 - acc: 1.0000 - val_loss: 1.1901e-05 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 9.3529e-06 - acc: 1.0000 - val_loss: 8.7733e-06 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 6.8571e-06 - acc: 1.0000 - val_loss: 6.5255e-06 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 5.0939e-06 - acc: 1.0000 - val_loss: 4.9340e-06 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 3.8159e-06 - acc: 1.0000 - val_loss: 3.7658e-06 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 2.8823e-06 - acc: 1.0000 - val_loss: 2.8883e-06 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 2.1961e-06 - acc: 1.0000 - val_loss: 2.2349e-06 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 1.6872e-06 - acc: 1.0000 - val_loss: 1.7478e-06 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 1.3054e-06 - acc: 1.0000 - val_loss: 1.3757e-06 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 1.0185e-06 - acc: 1.0000 - val_loss: 1.0887e-06 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 8.0296e-07 - acc: 1.0000 - val_loss: 8.7126e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 6.3975e-07 - acc: 1.0000 - val_loss: 7.0290e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0108 - acc: 0.9982 - val_loss: 4.2811e-07 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 3.9851e-07 - acc: 1.0000 - val_loss: 4.0881e-07 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 3.7699e-07 - acc: 1.0000 - val_loss: 3.8350e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 3.3764e-07 - acc: 1.0000 - val_loss: 2.9048e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 2.0417e-07 - acc: 1.0000 - val_loss: 1.6232e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 1.4391e-07 - acc: 1.0000 - val_loss: 1.3643e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 1.3064e-07 - acc: 1.0000 - val_loss: 1.2897e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 1.2588e-07 - acc: 1.0000 - val_loss: 1.2524e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 1.2360e-07 - acc: 1.0000 - val_loss: 1.2339e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 1.2229e-07 - acc: 1.0000 - val_loss: 1.2228e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 1.2151e-07 - acc: 1.0000 - val_loss: 1.2158e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 1.2098e-07 - acc: 1.0000 - val_loss: 1.2107e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 1.2061e-07 - acc: 1.0000 - val_loss: 1.2075e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 1.2036e-07 - acc: 1.0000 - val_loss: 1.2047e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 1.2016e-07 - acc: 1.0000 - val_loss: 1.2028e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 1.2001e-07 - acc: 1.0000 - val_loss: 1.2013e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 1.1988e-07 - acc: 1.0000 - val_loss: 1.1998e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 1.1979e-07 - acc: 1.0000 - val_loss: 1.1989e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 1.1971e-07 - acc: 1.0000 - val_loss: 1.1981e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 1.1966e-07 - acc: 1.0000 - val_loss: 1.1976e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0243 - acc: 0.9954 - val_loss: 1.4707e-05 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 7.4123e-06 - acc: 1.0000 - val_loss: 3.7184e-06 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 2.0927e-06 - acc: 1.0000 - val_loss: 1.3957e-06 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 9.0569e-07 - acc: 1.0000 - val_loss: 7.4054e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 5.2204e-07 - acc: 1.0000 - val_loss: 4.7465e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 3.5394e-07 - acc: 1.0000 - val_loss: 3.4022e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 2.6707e-07 - acc: 1.0000 - val_loss: 2.6700e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 2.1796e-07 - acc: 1.0000 - val_loss: 2.2200e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 1.8737e-07 - acc: 1.0000 - val_loss: 1.9282e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 1.6743e-07 - acc: 1.0000 - val_loss: 1.7316e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 1.5394e-07 - acc: 1.0000 - val_loss: 1.5920e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 1.4453e-07 - acc: 1.0000 - val_loss: 1.4928e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 1.3803e-07 - acc: 1.0000 - val_loss: 1.4205e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 1.3330e-07 - acc: 1.0000 - val_loss: 1.3686e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 1.2985e-07 - acc: 1.0000 - val_loss: 1.3297e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 1.2733e-07 - acc: 1.0000 - val_loss: 1.3004e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 1.2541e-07 - acc: 1.0000 - val_loss: 1.2777e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 1.2398e-07 - acc: 1.0000 - val_loss: 1.2605e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 1.2295e-07 - acc: 1.0000 - val_loss: 1.2477e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 1.2213e-07 - acc: 1.0000 - val_loss: 1.2375e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0322 - acc: 0.9957 - val_loss: 1.1105e-05 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 2.2877e-06 - acc: 1.0000 - val_loss: 1.0453e-06 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 4.9070e-07 - acc: 1.0000 - val_loss: 5.0768e-07 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 2.8034e-07 - acc: 1.0000 - val_loss: 3.4017e-07 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 2.0717e-07 - acc: 1.0000 - val_loss: 2.6339e-07 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 1.7322e-07 - acc: 1.0000 - val_loss: 2.1917e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 1.5477e-07 - acc: 1.0000 - val_loss: 1.9225e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.4382e-07 - acc: 1.0000 - val_loss: 1.7426e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.3674e-07 - acc: 1.0000 - val_loss: 1.6206e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.3192e-07 - acc: 1.0000 - val_loss: 1.5241e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.2860e-07 - acc: 1.0000 - val_loss: 1.4595e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.2627e-07 - acc: 1.0000 - val_loss: 1.4026e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2456e-07 - acc: 1.0000 - val_loss: 1.3614e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.2331e-07 - acc: 1.0000 - val_loss: 1.3309e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2240e-07 - acc: 1.0000 - val_loss: 1.3049e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2169e-07 - acc: 1.0000 - val_loss: 1.2854e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2115e-07 - acc: 1.0000 - val_loss: 1.2689e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2076e-07 - acc: 1.0000 - val_loss: 1.2563e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2046e-07 - acc: 1.0000 - val_loss: 1.2462e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.2021e-07 - acc: 1.0000 - val_loss: 1.2379e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 0.0823 - acc: 0.9940 - val_loss: 9.4241e-04 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 4.8754e-04 - acc: 1.0000 - val_loss: 2.3810e-04 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.5525e-04 - acc: 1.0000 - val_loss: 1.0014e-04 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 7.1641e-05 - acc: 1.0000 - val_loss: 5.2316e-05 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 3.9060e-05 - acc: 1.0000 - val_loss: 3.0576e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 2.3344e-05 - acc: 1.0000 - val_loss: 1.9175e-05 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.4866e-05 - acc: 1.0000 - val_loss: 1.2689e-05 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 9.8943e-06 - acc: 1.0000 - val_loss: 8.6879e-06 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 6.7924e-06 - acc: 1.0000 - val_loss: 6.1109e-06 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 4.7720e-06 - acc: 1.0000 - val_loss: 4.3824e-06 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 3.4185e-06 - acc: 1.0000 - val_loss: 3.2011e-06 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 2.4957e-06 - acc: 1.0000 - val_loss: 2.3875e-06 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.8495e-06 - acc: 1.0000 - val_loss: 1.7976e-06 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.3909e-06 - acc: 1.0000 - val_loss: 1.3745e-06 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.0590e-06 - acc: 1.0000 - val_loss: 1.0660e-06 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 8.1657e-07 - acc: 1.0000 - val_loss: 8.2921e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 6.4077e-07 - acc: 1.0000 - val_loss: 6.6166e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 5.1016e-07 - acc: 1.0000 - val_loss: 5.3044e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 4.1245e-07 - acc: 1.0000 - val_loss: 4.3381e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 3.3991e-07 - acc: 1.0000 - val_loss: 3.6104e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0188 - acc: 0.9956 - val_loss: 4.0859e-05 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 1.8598e-05 - acc: 1.0000 - val_loss: 9.6812e-06 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 5.5753e-06 - acc: 1.0000 - val_loss: 4.1674e-06 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 2.5819e-06 - acc: 1.0000 - val_loss: 2.2812e-06 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 1.4659e-06 - acc: 1.0000 - val_loss: 1.4425e-06 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 9.4352e-07 - acc: 1.0000 - val_loss: 9.9761e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 6.5789e-07 - acc: 1.0000 - val_loss: 7.2841e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 4.8600e-07 - acc: 1.0000 - val_loss: 5.5805e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 3.7631e-07 - acc: 1.0000 - val_loss: 4.4381e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 3.0326e-07 - acc: 1.0000 - val_loss: 3.6071e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 2.5318e-07 - acc: 1.0000 - val_loss: 3.0266e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 2.1818e-07 - acc: 1.0000 - val_loss: 2.6135e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 1.9313e-07 - acc: 1.0000 - val_loss: 2.2886e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 1.7486e-07 - acc: 1.0000 - val_loss: 2.0554e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 1.6148e-07 - acc: 1.0000 - val_loss: 1.8750e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 1.5162e-07 - acc: 1.0000 - val_loss: 1.7353e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 1.4412e-07 - acc: 1.0000 - val_loss: 1.6261e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 1.3849e-07 - acc: 1.0000 - val_loss: 1.5400e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 1.3422e-07 - acc: 1.0000 - val_loss: 1.4760e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 1.3097e-07 - acc: 1.0000 - val_loss: 1.4221e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0148 - acc: 0.9979 - val_loss: 3.6851e-05 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 1.7000e-05 - acc: 1.0000 - val_loss: 8.5477e-06 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 5.1816e-06 - acc: 1.0000 - val_loss: 3.7489e-06 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 2.4765e-06 - acc: 1.0000 - val_loss: 2.0857e-06 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 1.4295e-06 - acc: 1.0000 - val_loss: 1.3182e-06 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 9.2361e-07 - acc: 1.0000 - val_loss: 9.0895e-07 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 6.4674e-07 - acc: 1.0000 - val_loss: 6.6457e-07 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 4.8053e-07 - acc: 1.0000 - val_loss: 5.1069e-07 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 3.7454e-07 - acc: 1.0000 - val_loss: 4.0897e-07 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 3.0285e-07 - acc: 1.0000 - val_loss: 3.3477e-07 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 2.5333e-07 - acc: 1.0000 - val_loss: 2.8237e-07 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 2.1862e-07 - acc: 1.0000 - val_loss: 2.4435e-07 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 1.9342e-07 - acc: 1.0000 - val_loss: 2.1599e-07 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 1.7519e-07 - acc: 1.0000 - val_loss: 1.9464e-07 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 1.6170e-07 - acc: 1.0000 - val_loss: 1.7849e-07 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 1.5163e-07 - acc: 1.0000 - val_loss: 1.6592e-07 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 1.4414e-07 - acc: 1.0000 - val_loss: 1.5649e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 1.3840e-07 - acc: 1.0000 - val_loss: 1.4888e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 1.3407e-07 - acc: 1.0000 - val_loss: 1.4299e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 1.3080e-07 - acc: 1.0000 - val_loss: 1.3840e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.0315 - acc: 0.9946 - val_loss: 4.1823e-04 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 2.3255e-04 - acc: 1.0000 - val_loss: 1.2957e-04 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 8.7547e-05 - acc: 1.0000 - val_loss: 6.1801e-05 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 4.4723e-05 - acc: 1.0000 - val_loss: 3.5361e-05 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 2.6342e-05 - acc: 1.0000 - val_loss: 2.2245e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.6840e-05 - acc: 1.0000 - val_loss: 1.5037e-05 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 1.1362e-05 - acc: 1.0000 - val_loss: 1.0562e-05 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 7.9522e-06 - acc: 1.0000 - val_loss: 7.6573e-06 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 5.7046e-06 - acc: 1.0000 - val_loss: 5.6955e-06 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 4.1783e-06 - acc: 1.0000 - val_loss: 4.3160e-06 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 3.1045e-06 - acc: 1.0000 - val_loss: 3.2898e-06 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 2.3374e-06 - acc: 1.0000 - val_loss: 2.5469e-06 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.7803e-06 - acc: 1.0000 - val_loss: 1.9973e-06 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 1.3705e-06 - acc: 1.0000 - val_loss: 1.5739e-06 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 1.0653e-06 - acc: 1.0000 - val_loss: 1.2558e-06 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 8.3732e-07 - acc: 1.0000 - val_loss: 1.0115e-06 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 6.6625e-07 - acc: 1.0000 - val_loss: 8.2268e-07 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 5.3786e-07 - acc: 1.0000 - val_loss: 6.7741e-07 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 4.3977e-07 - acc: 1.0000 - val_loss: 5.6298e-07 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 3.6502e-07 - acc: 1.0000 - val_loss: 4.7242e-07 - val_acc: 1.0000\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0547 - acc: 0.9953 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 9.4187e-04 - acc: 1.0000 - val_loss: 5.2167e-04 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 3.7084e-04 - acc: 1.0000 - val_loss: 2.5916e-04 - val_acc: 1.0000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.9767e-04 - acc: 1.0000 - val_loss: 1.5266e-04 - val_acc: 1.0000\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.2062e-04 - acc: 1.0000 - val_loss: 9.9087e-05 - val_acc: 1.0000\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 7.9410e-05 - acc: 1.0000 - val_loss: 6.8112e-05 - val_acc: 1.0000\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 5.4810e-05 - acc: 1.0000 - val_loss: 4.8586e-05 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 3.9064e-05 - acc: 1.0000 - val_loss: 3.5572e-05 - val_acc: 1.0000\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 2.8521e-05 - acc: 1.0000 - val_loss: 2.6621e-05 - val_acc: 1.0000\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 2.1198e-05 - acc: 1.0000 - val_loss: 2.0266e-05 - val_acc: 1.0000\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.5940e-05 - acc: 1.0000 - val_loss: 1.5558e-05 - val_acc: 1.0000\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 1.2102e-05 - acc: 1.0000 - val_loss: 1.2038e-05 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 9.2520e-06 - acc: 1.0000 - val_loss: 9.3887e-06 - val_acc: 1.0000\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 7.1223e-06 - acc: 1.0000 - val_loss: 7.3996e-06 - val_acc: 1.0000\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 5.5102e-06 - acc: 1.0000 - val_loss: 5.8415e-06 - val_acc: 1.0000\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 4.2779e-06 - acc: 1.0000 - val_loss: 4.6294e-06 - val_acc: 1.0000\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 3.3364e-06 - acc: 1.0000 - val_loss: 3.6916e-06 - val_acc: 1.0000\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 2.6183e-06 - acc: 1.0000 - val_loss: 2.9621e-06 - val_acc: 1.0000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 2.0614e-06 - acc: 1.0000 - val_loss: 2.3804e-06 - val_acc: 1.0000\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 1.6302e-06 - acc: 1.0000 - val_loss: 1.9319e-06 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Architecture</th>\n",
              "      <th>Epoch time</th>\n",
              "      <th>Accuracy Train</th>\n",
              "      <th>Accuracy Test</th>\n",
              "      <th>Loss Train</th>\n",
              "      <th>Loss Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>784-256-256-256-256-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1920928955078125e-07</td>\n",
              "      <td>1.1920928955078125e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>784-128-128-128-128-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.199682575437085e-07</td>\n",
              "      <td>1.1942386636292213e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>784-64-64-64-64-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2025038532404627e-07</td>\n",
              "      <td>1.2082457969881945e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>784-12-12-12-12-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.4509509328490822e-07</td>\n",
              "      <td>1.517779495316063e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>784-256-256-256-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1928478880539236e-07</td>\n",
              "      <td>1.1928081516998645e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>784-128-128-128-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2095173860871e-07</td>\n",
              "      <td>1.206219245432294e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>784-64-64-64-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2006759756483612e-07</td>\n",
              "      <td>1.2333991935520318e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>784-12-12-12-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.698952008970082e-07</td>\n",
              "      <td>7.028974341665162e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>784-256-256-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1962652273875088e-07</td>\n",
              "      <td>1.1975765355600743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>784-128-128-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2181302442210533e-07</td>\n",
              "      <td>1.2374522316349611e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>784-64-64-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.201023677519212e-07</td>\n",
              "      <td>1.2378696451378345e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>784-12-12-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0986857451959317e-07</td>\n",
              "      <td>3.6104356809119054e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>784-256-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2958254590860938e-07</td>\n",
              "      <td>1.4220517505236786e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>784-128-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2938784180581328e-07</td>\n",
              "      <td>1.38396340298641e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>784-64-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.333180156005255e-07</td>\n",
              "      <td>4.7241672800737436e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>784-12-2</td>\n",
              "      <td>1secs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.4496984953363305e-06</td>\n",
              "      <td>1.931938025472846e-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Architecture Epoch time Accuracy Train Accuracy Test  \\\n",
              "0   784-256-256-256-256-2      1secs            1.0           1.0   \n",
              "1   784-128-128-128-128-2      1secs            1.0           1.0   \n",
              "2       784-64-64-64-64-2      1secs            1.0           1.0   \n",
              "3       784-12-12-12-12-2      1secs            1.0           1.0   \n",
              "4       784-256-256-256-2      1secs            1.0           1.0   \n",
              "5       784-128-128-128-2      1secs            1.0           1.0   \n",
              "6          784-64-64-64-2      1secs            1.0           1.0   \n",
              "7          784-12-12-12-2      1secs            1.0           1.0   \n",
              "8           784-256-256-2      1secs            1.0           1.0   \n",
              "9           784-128-128-2      1secs            1.0           1.0   \n",
              "10            784-64-64-2      1secs            1.0           1.0   \n",
              "11            784-12-12-2      1secs            1.0           1.0   \n",
              "12              784-256-2      1secs            1.0           1.0   \n",
              "13              784-128-2      1secs            1.0           1.0   \n",
              "14               784-64-2      1secs            1.0           1.0   \n",
              "15               784-12-2      1secs            1.0           1.0   \n",
              "\n",
              "                Loss Train               Loss Test  \n",
              "0   1.1920928955078125e-07  1.1920928955078125e-07  \n",
              "1    1.199682575437085e-07  1.1942386636292213e-07  \n",
              "2   1.2025038532404627e-07  1.2082457969881945e-07  \n",
              "3   1.4509509328490822e-07   1.517779495316063e-07  \n",
              "4   1.1928478880539236e-07  1.1928081516998645e-07  \n",
              "5      1.2095173860871e-07   1.206219245432294e-07  \n",
              "6   1.2006759756483612e-07  1.2333991935520318e-07  \n",
              "7    5.698952008970082e-07   7.028974341665162e-07  \n",
              "8   1.1962652273875088e-07  1.1975765355600743e-07  \n",
              "9   1.2181302442210533e-07  1.2374522316349611e-07  \n",
              "10   1.201023677519212e-07  1.2378696451378345e-07  \n",
              "11  3.0986857451959317e-07  3.6104356809119054e-07  \n",
              "12  1.2958254590860938e-07  1.4220517505236786e-07  \n",
              "13  1.2938784180581328e-07    1.38396340298641e-07  \n",
              "14   3.333180156005255e-07  4.7241672800737436e-07  \n",
              "15  1.4496984953363305e-06   1.931938025472846e-06  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgg4IVpbfeZ9",
        "colab_type": "text"
      },
      "source": [
        "# Building CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14B_Jf-yfeZ-",
        "colab_type": "text"
      },
      "source": [
        "## Load MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZgVJL4FfeaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load MNIST data\n",
        "from sklearn.model_selection import train_test_split\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "img_rows, img_cols = 28, 28\n",
        "number_of_classes = 10\n",
        "print(f\"X before flatten train      shape: {X_train.shape}\")\n",
        "print(f\"X before flatten validation shape: {X_valid.shape}\")\n",
        "print(f\"X before flatten test       shape: {X_test.shape}\")\n",
        "\n",
        "#Tensorflow prefers to work with float32 data type. \n",
        "#So the next step is to cast data. \n",
        "# Also let's have our data in [0; 1]interval; it's common choice for grayscale images.\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_valid = X_valid.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_valid /= 255\n",
        "X_test /= 255\n",
        "print(f\"X train      shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"X validation shape: {X_valid.shape},  {y_valid.shape}\")\n",
        "print(f\"X test       shape: {X_test.shape},  {y_test.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bij5pnuXfeaC",
        "colab_type": "text"
      },
      "source": [
        "### Input image Data reshaping 28x28x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuttMKFjfeaC",
        "colab_type": "text"
      },
      "source": [
        "First of all, let's predefine image parameters:\n",
        "* **img_rows, img_cols** $-$ 2D dimension of a pictures; for MNIST it is $28 \\times 28$\n",
        "* **nb_classes** $-$ number of classes (digits in our case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQnhxmXqfeaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = 28, 28\n",
        "nb_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYHDDIqofeaG",
        "colab_type": "text"
      },
      "source": [
        "Theano and Tensorflow both are tensor-based libraries. It means that all objects inside it, all inputs and outputs are **tensors**. One can treat tensor as a simple multidimensional array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdgzURTEfeaH",
        "colab_type": "text"
      },
      "source": [
        "The thing that is different in Theano and Tensorflow is order of these dimensions inside tensor.\n",
        "\n",
        "With Theano yo're going to have 4-dimensional tensor with the following dimensions: **(Objects, Channels, Image rows,Image columns)**. Assume that $\\text{X_train}$ is our tensor. Then $\\text{X_train}[0]$ gives you one trainig object - it is an image with few channels in general case. $\\text{X_train}[0][0]$ gives you the first channel of the first object. And so on. The logic of tensors should be clear now.\n",
        "\n",
        "In Tensorflow the order is the following: **(Objects, Image rows,Image columns, Channels)**\n",
        "\n",
        "Thus we need to check what dimension order do we have and reshape our tensor in accordance with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1EOoSrKfeaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if K.image_dim_ordering() == 'th':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_valid = X_valid.reshape(X_valid.shape[0], 1, img_rows, img_cols)\n",
        "    X_test  = X_test.reshape( X_test.shape[0],  1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoxWX0uAfeaK",
        "colab_type": "text"
      },
      "source": [
        "### Setup the MNIST Target data (OHE)\n",
        "Setup the MNIST data. Here we use  **digits 0 to 9**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKevExHmfeaM",
        "colab_type": "text"
      },
      "source": [
        "Convert labels into [One-Hot Encoding](https://en.wikipedia.org/wiki/One-hot) because we're going to learn them through the softmax layer of CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f9IakJYfeaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(y_train, number_of_classes)\n",
        "y_valid = to_categorical(y_valid, number_of_classes)\n",
        "y_test  = to_categorical(y_test, number_of_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPfM8MkRfeaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"X train      shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"X validation shape: {X_valid.shape},  {y_valid.shape}\")\n",
        "print(f\"X test       shape: {X_test.shape},  {y_test.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kotfekbOfeaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbeL6DLHfeaV",
        "colab_type": "text"
      },
      "source": [
        "## Lay out the CNN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL6iFrGzfeaY",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to build the model step-by-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6dZsRxfeaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ6Dhdyafeag",
        "colab_type": "text"
      },
      "source": [
        "Out model is going to be *Sequential* which means that every new added layer will be automatically connected to the previous one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-o8EWh1feag",
        "colab_type": "text"
      },
      "source": [
        "Firstly, let's define hyperparameters of the network:\n",
        "* **filters** $-$ number of filters (or kernels) to use in every layer; in fact this is the same as having multiple channels in the image\n",
        "* **pool_size** $-$ size of the pooling window\n",
        "* **kernel_size** $-$ size of the convolutional filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKLL-cspfeah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = 32\n",
        "kernel_size = (3, 3)\n",
        "pool_size = (2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7RasV8Ufeal",
        "colab_type": "text"
      },
      "source": [
        "Now let's add first layer of the network. It is 2D Convolutional layer. Only unexplained thing here is *padding*. This is the parameter that defines how should we pad the data after applying convolutions. $\\text{padding} = \\text{'valid'}$ means that we're not going to pad images and the dimension of it is going to shrink from layer to layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQSVewmifeam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.add(Convolution2D(filters=filters, \n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=\"valid\",\n",
        "                            input_shape=input_shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs1SOzrYfear",
        "colab_type": "text"
      },
      "source": [
        "Next step is to add nonlinearity to enable our network to learn complex dependencies. We're going to use ReLU activation function because it is less exposed to vanishing gradient problem and faster to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y05WkORfear",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kBwZ023feat",
        "colab_type": "text"
      },
      "source": [
        "Let's stack one more Convolution layer on top of that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBnwL7Ijfeaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.add(Convolution2D(filters=filters, \n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=\"valid\"))\n",
        "model_cnn.add(Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCwEVPJfeay",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to apply pooling. Note that the strategies of combining convolutional and pooling layers may be different. For further details see [here](http://cs231n.stanford.edu/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzQJa3VZfeaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.add(MaxPooling2D(pool_size=pool_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94w-bATUfea6",
        "colab_type": "text"
      },
      "source": [
        "At this point we consider that we've already distinguish some meaningful features from the pictures. So it's time to classify them. For that purpose the common approach is to append fully-connected part. \n",
        "\n",
        "But before that we need to pull all the obtained feature into one vector so that one object has 1D-vector of features. It is done by means of $\\text{Flatten}$ layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zMeUky8fea7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2mkwmTHfea_",
        "colab_type": "text"
      },
      "source": [
        "Now let's add FC part with the [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnq02GmffebA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.add(Dropout(0.5))\n",
        "model_cnn.add(Dense(128, activation=\"relu\"))\n",
        "model_cnn.add(Dropout(0.5))\n",
        "model_cnn.add(Dense(nb_classes, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTNtcPpvfebD",
        "colab_type": "text"
      },
      "source": [
        "The final layer here is usual $\\text{Softmax}$ with the number of classe. So as the output of the network we observe the probability of each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvQRbThafebD",
        "colab_type": "text"
      },
      "source": [
        "Now let's compile our model.\n",
        "* **optimizer** $-$ here we use accelerated gradient descent algorithm with special adaptive way of choosing learning rate; for more details see this great [overview](http://sebastianruder.com/optimizing-gradient-descent/) of gradient descent optimization algorithms.\n",
        "* **loss** $-$ usual choice for multiclass classification is softmax output layer in combination with categotical crossentropy loss function which is\n",
        "$$\n",
        "\\mathcal{L}(\\text{true}, \\text{pred}) = -\\sum_{j=1}^{k}\\text{true}_j \\cdot \\log \\{\\text{pred}_j\\}\n",
        "$$\n",
        "* **metrics** $-$ additional metrics that we're going to trace while training; it doesn't influence training process at all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQtEylBIfebE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AQ7WsksfebF",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at our final model now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4GQGp7OfebF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzaP34BxfebH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVG(model_to_dot(model_cnn, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUD95F9_febI",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shiOrHNlfebI",
        "colab_type": "text"
      },
      "source": [
        "Training parameters are the following:\n",
        "* nb_epoch $-$ number of epochs to train. here we choose 12; one may condiser using some stopping criterias\n",
        "* **batch_size** $-$ parameter that controls how frequent do we update gradient; with $\\text{batch_size}=1$ optimization is nothing but pure Stohastic Gradient Descent (update gradient after passing each one object); with $\\text{batch_size}=\\textit{number of objects}$ it will be usual Gradient Descent which updates gradient only after passing all objects. Choosing value between this two one can control speed and convergence of training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqHPorsrfebL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfOssN6GfebM",
        "colab_type": "text"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkbvhi4BfebN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "hist = model_cnn.fit(X_train, y_train, \n",
        "                     batch_size=batch_size, \n",
        "                     epochs=epochs,\n",
        "                     validation_data=(X_valid, y_valid))\n",
        "end = time.time()\n",
        "seconds_per_epoch = f\"{(end - start)/epochs:.4}\"\n",
        "print(f\"seconds_per_epoch: {seconds_per_epoch}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWEsuk1wfebR",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJQ-FQFAfebU",
        "colab_type": "text"
      },
      "source": [
        "Visualization of learning process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp8kCKA5febV",
        "colab_type": "code",
        "outputId": "a64f90b7-a6a9-4a4e-9407-1edbcedd7f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.suptitle(\"CNN model training\", fontsize=18)\n",
        "plt.subplot(121)\n",
        "plt.plot(hist.history[\"loss\"], label=\"Train\")\n",
        "plt.plot(hist.history[\"val_loss\"], label=\"Validation\")\n",
        "plt.grid(\"on\")\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"Crossentropy\", fontsize=14)\n",
        "plt.title(\"Crossentropy CXE\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.subplot(122)\n",
        "plt.plot(hist.history[\"acc\"], label=\"Train\")\n",
        "plt.grid(\"on\")\n",
        "plt.plot(hist.history[\"val_acc\"], label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.ylim([0.88, 1.0]);"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKIAAAIkCAYAAAAtXYN1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5sHOt1-febZ",
        "colab_type": "text"
      },
      "source": [
        "Final evaluation of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE7OMyBFfebZ",
        "colab_type": "code",
        "outputId": "36ffc807-449d-4825-b60c-25bfa3a748e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# add the result of this experiment to the log book\n",
        "exp_name = \"CNN-784_32x3x3-32_3x3-128-10\" # experiment name\n",
        "try:\n",
        "    expLog\n",
        "except NameError:\n",
        "    expLog = pd.DataFrame(columns=[\"exp_name\",\"epoch (secs)\",\"Epochs\",\"Train CXE Loss\", \"Train Acc\", \"Validation CXE Loss\", \"Validation  Acc\",\n",
        "                    \"Test CXE Loss\", \"Test  Accuracy\"])\n",
        "    \n",
        "# Add a experiment results to the experiment log\n",
        "model = model_cnn\n",
        "expLog.loc[len(expLog)] = [f\"{exp_name}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train, y_train, verbose=0), \n",
        "                   model.evaluate(X_valid, y_valid, verbose=0),\n",
        "                   model.evaluate(X_test,  y_test, verbose=1)], -1), 3))\n",
        "expLog"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b9366f60a8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Add a experiment results to the experiment log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m expLog.loc[len(expLog)] = [f\"{exp_name}\", seconds_per_epoch, epochs] + list(np.round(np.reshape([model.evaluate(X_train, y_train, verbose=0), \n\u001b[1;32m     11\u001b[0m                    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_cnn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mNFhXwdgVYh",
        "colab_type": "text"
      },
      "source": [
        "# Task: choose a better CNN architecture\n",
        "\n",
        "\n",
        "## Task 5.1 : choose a better CNN architecture\n",
        "\n",
        "Search for a good CNN architecture to model the MNIST problem.\n",
        "\n",
        "It is up to you what the model will be. Here are some things you need to decide:\n",
        "* how many convolutional layers?\n",
        "* what spatial size will your convolutions be?\n",
        "* how many channels will your convolutions be?\n",
        "* what nonlinearity will you use?\n",
        "* will you use pooling? what type?\n",
        "* how many fully-connected layers will you have?\n",
        "* will you use dropout?\n",
        "* what batch size?\n",
        "\n",
        "Keras provides a special layer called `Flatten` to flatten the convolutional features into a vector before the fully-connected layers. You should look at the documentation for Keras's convolutional layers: http://keras.io/layers/convolutional/. In particular, you may want to look at `Convolution2D`, `MaxPooling2D`, `AveragePooling2D`, `Flatten`, and `Dropout`. For this problem, you make want to use the `'rmsprop'` optimizer - it is an algorithm that adapts the learning rate during learning for you automatically.\n",
        "\n",
        "\n",
        "\n",
        "**Please paste your response here using the following format:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADRlm4D2gXwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW9Quu3lghrE",
        "colab_type": "text"
      },
      "source": [
        "## Task 5.2 : with 500 training examples\n",
        "\n",
        "Use only 500 examples per class (recall we  have only 10 classes) and find the best architecture. \n",
        "HINTs:\n",
        "* you can do  stratified sampling OR just random selection of 5,000 examples\n",
        "* for inspiration, see [here](https://mclguide.readthedocs.io/en/latest/sklearn/multiclass.html)\n",
        "* For more details see: [sklearn.model_selection.train_test_split manual page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "  * `stratify` : array-like or None (default=None)\n",
        "    If not None, data is split in a stratified fashion, using this as the class labels.\n",
        "\n",
        "```python\n",
        "# load MNIST data\n",
        "from sklearn.model_selection import train_test_split\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
        "                                                     test_size=55000./60000., random_state=42, \n",
        "                                                     stratify=y_train)\n",
        "img_rows, img_cols = 28, 28\n",
        "number_of_classes = 10\n",
        "print(f\"X before flatten train      shape: {X_train.shape}\")\n",
        "print(f\"X before flatten validation shape: {X_valid.shape}\")\n",
        "print(f\"X before flatten test       shape: {X_test.shape}\")\n",
        "print(f\"All data: {np.bincount(y_train) / float(len(y_train))}\")\n",
        "```\n",
        "\n",
        "**Please paste your response here:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCLhwC5Bfebg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Task 5.3 : Odd versus Even Classifier\n",
        "\n",
        "Build a binary classifier that classifiers odd numbers versus even numbers. Again, try to find the best architecture.\n",
        "\n",
        "\n",
        "**Please paste your response here:** \n",
        "* Best archtecture is:  \n",
        "* Test accuracy: \n",
        "* Training time per epoch in seconds is : \n",
        "\n",
        "\n",
        "## 5.4 Homework Submission Instructions:\n",
        "\n",
        "\n",
        "\n",
        "1. Please fill in your best architecture for each of the three homework tasks 5.1, 5.2, 5.3: \n",
        "https://docs.google.com/spreadsheets/d/11Ot_v7EKmBoNgXBEjCsN_S7tYtfa5OXFLQoH62JCf1g/edit?usp=sharing\n",
        "  * NOTE: that each task has a SEPARATE table/tab in the above spreadsheet.\n",
        "  * The winner in each task gets a special prize!\n",
        "* Please upload your notebook to the following Google Drive folder (you save directly or just can drag and drop):\n",
        "  * https://drive.google.com/drive/folders/18Ng3PfnVBhrT82s_rQ_-WylE5rN7f9ej?usp=sharing\n",
        "* Your homework submission is now due tomorrow (Friday) morning at 8AM (that will give you an extra 12 hours!!)\n",
        "* Please send the following by EMAIL to James.Shanahan@gmail.com with the subject \"Sichuan2019: MNIST Best MLP Architecture\"\n",
        "\n",
        "  * Link to your own Google Colab notebook where you did the experiments\n",
        "  * All the text + your response from this cell\n",
        "  * A screenshot of your expLog table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmZmLjzfebh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpFCWS5nfebo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}